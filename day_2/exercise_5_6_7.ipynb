{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior decoding\n",
    "\n",
    "Posterior decoding consists\n",
    "in picking state with the highest posterior for each position in the sequence independently; for \n",
    "each $i = 1,\\ldots,N$:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i^* = \\text{argmax}_{y_i \\in \\Lambda} P(Y_i=y_i | X = x).\n",
    "\\end{equation}\n",
    "\n",
    "The **sequence posterior distribution** is the probability of a particular\n",
    "hidden state sequence given that we have observed a particular\n",
    "sequence. Moreover, we will be interested in two other posteriors distributions:\n",
    "the **state posterior distribution**, corresponding to the\n",
    "probability of being in a given state in a certain position given the\n",
    "observed sequence; and the \\textbf{transition posterior distribution},\n",
    "which is the probability of making a particular transition, from position $i$ to\n",
    "$i+1$, given the observed sequence. \n",
    "\n",
    "They are formally defined as follows:\n",
    "\n",
    "- Sequence  Posterior\n",
    "$$P(Y=y|X=x) = \\frac{P(X=x,Y=y)}{P(X=x)}\n",
    "$$\n",
    "\n",
    "- State Posterior\n",
    "$$\n",
    "P(Y_i=y_i | X=x)\n",
    "$$\n",
    "\n",
    "- Transition Posterior\n",
    "$$\n",
    "P(Y_{i+1}=y_{i+1},Y_i=y_i| X=x)\n",
    "$$\n",
    "\n",
    "\n",
    "### Computing posteriors involves beeing able to compute $P(X=x)$\n",
    "To compute the posteriors, a first step is to be able to compute the \n",
    "likelihood of\n",
    "the sequence $P(X=x)$, which corresponds to summing the probability of all\n",
    "possible hidden state sequences.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Likelihood\\!:}\\;\\;\\;\\; P(X=x) = \\displaystyle \\sum_{y \\in \\Lambda^N} P(X=x,Y=y).\n",
    "\\end{equation}\n",
    "\n",
    "The number of possible hidden state sequences is exponential in the\n",
    "length of the sequence ($|\\Lambda|^N$),\n",
    " which makes the sum over all of them hard. \n",
    " In our simple\n",
    " example, there are $2^4 = 16$ paths, which we can actually explicitly enumerate\n",
    " and calculate their probability using Equation of the joint probability $P(x,y)$. But this is as far as it goes: for example, for Part-of-Speech\n",
    " tagging with a small tagset of 12 tags and a medium size\n",
    " sentence of length 10, there are $12^{10} = 61 917 364 224$ such\n",
    " paths. \n",
    " \n",
    " Yet, we must be able to compute this sum (sum over $y \\in \\Lambda^N$) to compute the above likelihood\n",
    "formula; this is called the inference problem. For sequence models, there is a well known dynamic programming algorithm,\n",
    "the **Forward-Backward** (FB) algorithm, which allows the computation\n",
    "to be performed in linear time, The runtime is linear with respect\n",
    "to the sequence length. More precisely, \n",
    "the runtime is $O(N|\\Lambda|^2)$. \n",
    "A naive enumeration would cost $O(|\\Lambda|^N)$.\n",
    "\n",
    "The FB algorithm relies on the independence of previous states\n",
    "assumption, which  \n",
    "is illustrated in the trellis view by having arrows only between consecutive states. \n",
    "The FB algorithm defines two auxiliary probabilities, the forward probability and the backward probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient forward probability computation\n",
    "\n",
    "The forward probability represents the probability that in position\n",
    "$i$ we are in state $Y_i = c_k$ and that we have observed $x_1,\\ldots,x_i$\n",
    "up to that position. Therefore, its mathematical expression is:\n",
    "\\begin{equation}\n",
    "\\mathbf{Forward \\ Probability\\!:}\\;\\;\\;\\;  \\mathrm{forward}(i, c_k) = P(Y_i = c_k, X_1=x_1,\\ldots, X_i = x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\mathrm{forward}(i, c_k)$ using all the forward computations \\{$\\mathrm{forward}(i -1, c)$ for $c \\in \\Lambda$\\}. In order to facilitate the notation of the following argument we will denote by $x_{i:j}$  the assignemnt $X_i = x_i, \\dots, X_j = x_j$. Therefore we can write   $\\mathrm{forward}(i, y_i) $ as $P( y_i, x_{1:i } ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( y_i, x_{1:i } ) =  \\sum_{y_{i-1} \\in \\Lambda} P( y_i ,y_{i-1}, x_{1:i } )  =  \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) \\cdot P(y_i  | y_{i-1},  x_{1:i-1 }) \\cdot P(y_{i-1},  x_{1:i-1 })  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Using the **Observation independence** and the **Independence of previous states** properties of the first order HMM we have $P( x_i  | y_i,  y_{i-1},  x_{1:i-1 } ) = P( x_i  | y_i) $ and $P(y_i  | y_{i-1},  x_{1:i-1 })  = P(y_i  | y_{i-1})  $. Therefore the previous equation can be written, \n",
    "for $i \\in \\{2,\\dots,N\\}$ (where $N$ is the length of the sequence), as \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathrm{forward}(i, y_i)  = \\sum_{y_{i-1} \\in \\Lambda} P( x_i  | y_i, ) \\cdot P(y_i  | y_{i-1}) \\cdot \\mathrm{forward}(i-1, y_{i-1})   \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The previous equation proves that  the forward probability can be defined by the\n",
    "following recurrence rule: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{forward}(1, c_k)&=& P_{\\text{init}}(c_k|\\text{start}) \\times P_{\\mathrm{emiss}}(x_1 | c_k)\n",
    " \\\\\n",
    " \\mathrm{forward}(i, c_k) &=& \\left(  \\sum_{c_l \\in \\Lambda} P_{\\mathrm{trans}}(c_k | c_l) \\times \\mathrm{forward}(i-1, c_l) \\right) \\times P_{\\mathrm{emiss}}(x_i | c_k) \n",
    " \\\\\n",
    "  \\mathrm{forward}(N+1, \\text{stop}) &=& \\sum_{c_l \\in \\Lambda} P_{\\text{final}}(\\text{ stop} | c_l) \\times \\mathrm{forward}(N, c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Using the forward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{forward}(N+1, \\text{ stop}).\n",
    "\\end{equation}\n",
    "\n",
    "Although the forward probability is enough to calculate the likelihood of a given sequence, we will also need the backward probability to calculate the state posteriors. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient backward probability computation\n",
    "\n",
    "\n",
    "\n",
    "The backward probability is similar to the forward probability, but operates in the inverse direction.\n",
    "It represents the probability of observing $x_{i+1},\\ldots,x_N$ from position $i+1$ up to $N$, given that at position $i$ we are at state $Y_i = c_l$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Backward \\ Probability\\!:}\\;\\;\\;\\;  \\text{backward}(i, c_l) = P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N | Y_i = c_l).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Using the independence assumptions of the HMM we can compute $\\text{backward}(i, c_k)$ using all the backward computations $\\text{backward}(i +1, c)$ for $c \\in \\Lambda$.\n",
    "\n",
    "Therefore we can write   $\\text{backward}(i, y_i) $ as $P( x_{i+1:N} | y_i ) $ and rewrite the forward expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  P( x_{i+1:N} | y_i ) =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+1:N}, y_{i+1} | y_i)  =  \\sum_{y_{i+1} \\in \\Lambda} P( x_{i+2:N} | y_i, y_{i+1}, x_{i+1}) \n",
    "   P( x_{i+1}, |  y_{i+1},  y_{i}) P( y_{i+1} | y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Using the previous equation we have proved that the backward probability can be defined by the following recurrence rule:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{backward}(N, c_l) &=& P_{\\text{final}}(\\text{stop} | c_l)  \\\\\n",
    "\\text{backward}(i, c_l) &=&  \\displaystyle \\sum_{c_k \\in \\Lambda} P_{\\text{trans}}(c_k | c_l) \\times \n",
    "\\text{backward}(i+1, c_k) \\times P_{\\text{emiss}}(x_{i+1} | c_k) \n",
    " \\\\\n",
    "  \\mathrm{backward}(0, \\text{start}) &=& \\sum_{c_k \\in \\Lambda} P_{\\mathrm{init}}(c_k | \\text{ start}) \\times \\mathrm{backward}(1, c_k) \\times P_{\\mathrm{emiss}}(x_{1} | c_k).\n",
    " \\end{eqnarray}\n",
    "\n",
    "Using the backward trellis one can compute the likelihood simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X=x) = \\mathrm{backward}(0, \\text{start}).\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward backward algorithm\n",
    "\n",
    "We have seen how we can compute the probability of a sequence $x$ using the the forward and backward probabilities by computing  $\\mathrm{forward}(N+1, \\text{ stop})$ and $ \\mathrm{backward}(0, \\text{ start})$ respectively. Moreover,  the probability of a sequence $x$ can be computed with both forward and backward probabilities at a particular position $i$. \n",
    "\n",
    "The probability of a  given sequence $x$ at any position $i$ in the sequence can be computed\n",
    "as follows:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  P(X=x) &=& \n",
    "  \\sum_{c_k \\in \\Lambda} P(X_1=x_1,\\ldots, X_N=x_N,Y_i=c_k)\\nonumber\\\\\n",
    "  & =&\n",
    "  \\sum_{c_k \\in \\Lambda} \n",
    "  \\underbrace{P(X_1=x_1,\\ldots, X_i=x_i, Y_i=c_k)}_{\\mathrm{forward}(i,c_k)} \\times \n",
    "  \\underbrace{P(X_{i+1}=x_{i+1},\\ldots, X_N=x_N| Y_i=c_k)}_{\\mathrm{backward}(i,c_k)}\\nonumber\\\\\n",
    "  &=& \\sum_{c_k \\in \\Lambda} \\mathrm{forward}(i,c_k) \\times \\mathrm{backward}(i,c_k).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "This equation will work for any choice of $i$. Although redundant, this fact is useful when implementing an\n",
    "HMM as a sanity check that the computations are being performed\n",
    "correctly, since one can compute this expression for several $i$; they should all yield the same value. \n",
    "\n",
    "The following pseudocode shows the the forward backward algorithm. \n",
    "\n",
    "<img src=\"../images_for_notebooks/day_2/fb_alg.png\"  style=\"max-width:100%; width: 50%\">\n",
    "\n",
    "The reader can notice that the $forward$ and $backward$ computations in the algorithm make use of $P_{emiss}$ and $P_{trans}$. There are a couple of details that should be taken into account if the reader wants to understand the algorithm using scores instead of probabilities.\n",
    "\n",
    "\n",
    "- $forward(i,\\hat{c})$  is computed using $P_{emiss}(x_i | \\hat{c})$ which does not depend on the sum over all possible states $c_k \\in  \\Lambda $. Therefore when taking the logarithm of the sum over all possible states the recurrence of the forward computations can be split as a sum of two logarithms.\n",
    "\n",
    "\n",
    "- $backward(i,\\hat{c})$  is computed using $ P_{\\text{trans}}(c_k | \\hat{c} )$ and $P_{\\text{emiss}}(x_{i+1} | c_k) $ both of  which  depend on $c_k$. Therefore when taking the logarithm of the sum the expression cannot be split as a sum of logarithms.\n",
    "\n",
    "\n",
    "\n",
    "Given the forward and backward probabilities, one can compute both the state\n",
    "and transition posteriors as follows:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{State \\ Posterior\\!:}\\;\\;\\;\\;  & P(Y_i = y_i| X=x) = \\frac{\\mathrm{forward}(i, y_i) \\times \n",
    " \\mathrm{backward}(i, y_i)}{P(X=x)}\\\\\n",
    " \\mathbf{Transition \\ Posterior\\!:}\\;\\;\\;\\; &\n",
    " P(Y_i = y_i, Y_{i+1} = y_{i+1} | X=x)= \\nonumber\\\\\n",
    " &\n",
    "   \\frac{\\mathrm{forward}(i, y_i) \\times \n",
    "   P_{\\mathrm{trans}}(y_{i+1}|y_i) \\times\n",
    "   P_{\\mathrm{emiss}}(x_{i+1}|y_{i+1}) \\times\n",
    " \\mathrm{backward}(i+1, y_{i+1})}{P(X=x)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical representation of these posteriors is illustrated in the following figure:\n",
    "\n",
    "<img src=\"../images_for_notebooks/day_2/ex_trellis.png\"  style=\"max-width:100%; width: 80%\">\n",
    "\n",
    "On the left it is shown that $\\mathrm{forward}(i, y_i)  \\times \\mathrm{backward}(i, y_i)$ returns the sum of all paths that contain the state $y_i$, weighted by $P(X=x)$; on the right we can see that \n",
    "\n",
    "$$\\mathrm{forward}(i, y_i) \\times P_{\\mathrm{trans}}(y_{i+1}|y_i) \\times P_{\\mathrm{emiss}}(x_{i+1}|y_{i+1}) \\times \\mathrm{backward}(i+1, y_{i+1})$$\n",
    "\n",
    "returns the same for all paths containing the edge from $y_i$ to $y_{i+1}$. Thus, these posteriors can be seen as the ratio of the number of paths that contain the given state or transition (weighted by $P(X=x)$) and the number of possible paths in the graph marginal.\n",
    "\n",
    "As a practical example, given that the person performs the sequence of actions $\\text{ walk} \\text{ walk} \\text{ shop} \\text{ clean}$, we want to know the probability of having been raining in the second day. The state posterior probability for this event can be seen as the probability that the sequence of actions above was generated by a sequence of weathers and where it was raining in the second day. In this case, the possible sequences would be all the sequences which have ${\\tt rainy}$ in the second position.\n",
    "\n",
    "\n",
    "Using the state posteriors, we are ready to perform posterior\n",
    "decoding. \n",
    "The strategy is to compute the state posteriors \n",
    "for each position $i \\in \\{1,\\ldots,N\\}$\n",
    "and each state $c_k \\in \\Lambda$, and \n",
    "then pick the arg-max at each position:\n",
    "\n",
    "$$\n",
    "{\\widehat y_i} := \\text{argmax}_{y_i \\in \\Lambda} P(Y_i=y_i| X=x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the hmm class\n",
    "\n",
    "The HMM class inherits from SequenceClassifier\n",
    "\n",
    "In the following exercise we will use the run_forward function which \n",
    "\n",
    "    def run_forward(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "            length = np.size(emission_scores, 0) # Length of the sequence.\n",
    "            num_states = np.size(initial_scores) # Number of states.\n",
    "\n",
    "            # Forward variables.\n",
    "            forward = np.zeros([length, num_states]) + logzero()\n",
    "\n",
    "            # Initialization.\n",
    "            forward[0,:] = emission_scores[0,:] + initial_scores\n",
    "\n",
    "            # Forward loop.\n",
    "            for pos in xrange(1,length):\n",
    "                for current_state in xrange(num_states):\n",
    "                    # Note the fact that multiplication in log domain turns a sum and sum turns a logsum\n",
    "                    forward[pos, current_state] = \\\n",
    "                            logsum(forward[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "                    forward[pos, current_state] += emission_scores[pos, current_state]\n",
    "\n",
    "            # Termination.\n",
    "            log_likelihood = logsum(forward[length-1,:] + final_scores)\n",
    "\n",
    "            return log_likelihood, forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5 \n",
    "\n",
    "Run the provided forward-backward algorithm on the first train sequence.\n",
    "Observe that both the forward and the backward passes give the same log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../../lxmls-toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[walk/rainy walk/sunny shop/sunny clean/sunny ,\n",
       " walk/rainy walk/rainy shop/rainy clean/sunny ,\n",
       " walk/sunny shop/sunny shop/sunny clean/sunny ]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "simple.train.seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:194: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:192: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:197: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rainy': 0, 'sunny': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.state_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean': 2, 'shop': 1, 'tennis': 3, 'walk': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.observation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40546511, -1.09861229])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75 ,  0.25 ],\n",
       "       [ 0.75 ,  0.25 ],\n",
       "       [ 0.25 ,  0.375],\n",
       "       [ 0.   ,  0.375]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(emission_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28768207, -1.38629436],\n",
       "       [-0.28768207, -1.38629436],\n",
       "       [-1.38629436, -0.98082925],\n",
       "       [       -inf, -0.98082925]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40546511, -1.09861229])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       -inf, -0.98082925])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.69314718,        -inf],\n",
       "        [-0.69314718, -0.47000363]],\n",
       "\n",
       "       [[-0.69314718,        -inf],\n",
       "        [-0.69314718, -0.47000363]],\n",
       "\n",
       "       [[-0.69314718,        -inf],\n",
       "        [-0.69314718, -0.47000363]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward:\n",
      "[[-0.69314718 -2.48490665]\n",
      " [-1.67397643 -2.58334672]\n",
      " [-3.75341798 -2.94017562]\n",
      " [       -inf -4.08740307]] \n",
      "\n",
      "\n",
      " Log-Likelihood = -5.06823232601\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores,final_scores, emission_scores)\n",
    "print \"forward:\\n\", forward, \"\\n\"\n",
    "print '\\n Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward :\n",
      "[[-4.41863845 -5.73879301]\n",
      " [-3.67819455 -3.88249502]\n",
      " [-2.65480569 -2.43166214]\n",
      " [       -inf -0.98082925]] \n",
      "\n",
      "Log-Likelihood = -5.06823232601\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print \"backward :\\n\", backward, \"\\n\"\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the node posteriors for the first training sequence (use the provided compute posteriors function), \n",
    "and look at the output. Note that the state posteriors are a proper probability distribution \n",
    "(the lines of the result sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, transition_posteriors, log_likelihood = hmm.compute_posteriors(initial_scores, transition_scores, final_scores, emission_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95738152  0.04261848]\n",
      " [ 0.75281282  0.24718718]\n",
      " [ 0.26184794  0.73815206]\n",
      " [ 0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print state_posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 2.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the posterior decode on the first test sequence, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny  \n",
      "\n",
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0  ])\n",
    "print \"Prediction test 0:\\n\\t\", y_pred, \"\\n\"\n",
    "print \"Truth test 0:\\n\\t\", simple.test.seq_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the second test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1:\n",
      "clean/rainy walk/rainy tennis/rainy walk/rainy \n",
      "Truth test 1:\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../lxmls-toolkit/lxmls/sequences/sequence_classifier.py:78: RuntimeWarning: invalid value encountered in subtract\n",
      "  state_posteriors[pos,:] -= log_likelihood\n",
      "../../lxmls-toolkit/lxmls/sequences/sequence_classifier.py:91: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  transition_posteriors[pos, state, prev_state] -= log_likelihood\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "# There are nan values in the backward and forward probabilites caused by\n",
    "# not having observed tennis\n",
    "\n",
    "print \"Prediction test 1:\"\n",
    "print y_pred\n",
    "print \"Truth test 1:\"\n",
    "print simple.test.seq_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong?\n",
    "\n",
    "**Note the observations for the second test sequence: the observation tennis was never seen at training time**, so the probability for it will be zero (no matter what state). This will make all possible state sequences have zero probability. As seen in the previous lecture, this is a problem with generative models, which can be corrected using smoothing (among other options).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the train supervised method to add smoothing:\n",
    "```\n",
    "   def train_supervised(self,sequence_list, smoothing):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0 with smoothing:\n",
      "\twalk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0:\n",
      "\twalk/rainy walk/sunny shop/sunny clean/sunny \n",
      "\n",
      "\n",
      "Prediction test 1 with smoothing:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "Truth test 1:\n",
      "\tclean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Prediction test 0 with smoothing:\"\n",
    "print \"\\t\",y_pred \n",
    "print \"Truth test 0:\"\n",
    "print \"\\t\",simple.test.seq_list[0]\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"\\n\"\n",
    "print \"Prediction test 1 with smoothing:\"\n",
    "print \"\\t\",y_pred\n",
    "print \"Truth test 1:\"\n",
    "print \"\\t\",simple.test.seq_list[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
