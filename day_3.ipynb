{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Learning Structured Predictors\n",
    "\n",
    "In Day 2, we focused on generative sequence classifiers - HMMs. Today's focus is on discriminative classifiers. Recall that:\n",
    "\n",
    "* **generative classifiers** try to model the probability distribution of the data, $P(X, Y)$;\n",
    "\n",
    "* **discriminative classifiers** only model the conditional probability of each class given the observed data, $P(Y\\,|\\,X)$.\n",
    "\n",
    "In Day 1, we implemented discriminative models for classification tasks. Today, we extend this concept to the classification of _sequential_ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be using two discriminative classifiers to do part-of-speech tagging:\n",
    "* Conditional Random Fields (CRF) and\n",
    "* Structured Perceptron.\n",
    "\n",
    "Your tasks for this lab session are:\n",
    "\n",
    "* to train a CRF model using two different sets of features (exercises 3.1 and 3.2); \n",
    "* to implement the structured perceptron algorithm (exercise 3.3); \n",
    "* to compare the performance of the Structured Perceptron with that of CRFs (exercise 3.4).\n",
    "\n",
    "\n",
    "**Therefore today's coding in centered in implementing the ```.perceptron_update``` method inside the  ```StructuredPerceptron``` class**. The **```class CRFBatch```** and the **```class CRFOnline```** are alredy implemented and will be used in Exercise 3.1 and 3.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../lxmls-toolkit')\n",
    "\n",
    "import lxmls\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc\n",
    "from lxmls.readers import pos_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Discriminative sequence models aim to solve the following:\n",
    "\n",
    "$$\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ P(Y=y\\,|\\,X=x)=\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \\boldsymbol{w}\\cdot\\boldsymbol{f}(x, y)$$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the model's weight vector, and $\\boldsymbol{f}(x, y)$ is a feature vector. Notice that now both $y$ and $x$ are $N$-dimensional vectors, whereas in Day 1, these variables were just scalar numbers.\n",
    "\n",
    "In Day 2, sequences were scored using the log-probability. On today's models we are still scoring the sequences; the only difference is the scores are now computed as the product of the weights with the feature vector:\n",
    "\n",
    "\n",
    "| score | Hidden Markov Models (Day 2) | Discriminative Models (Today) |\n",
    "| ------------------------------- | ---------------- | ---------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}$ | $\\log P(x_i\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i)$ |\n",
    "| $\\textrm{score}_\\textrm{init}$ | $\\log P(y_1\\,|\\,\\mathrm{start}) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}$ | $\\log P(y_{i+1}\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}$ | $\\log P(\\mathrm{stop}\\,|\\,y_N) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N)$ |\n",
    "\n",
    "Notice that the scores computed using the feature vector depend on two sequential values of the output variable, $y$, but may depend on the whole observated input, $x$. We can now rewrite the above expression as\n",
    "\n",
    "$$\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N) = \n",
    "\\\\\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\textrm{score}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\textrm{score}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\textrm{score}_\\textrm{trans}(i, x, y_i, y_{i+1}) +\n",
    "\\textrm{score}_\\textrm{final}(x, y_N)\n",
    "$$\n",
    "\n",
    "The reader can notice that feature vectors depend locally on the output variable. The features depend on \n",
    "\n",
    "- a single $y_i$ in the case of emission scores, initial scores and final scores.\n",
    "- or a pair  $y_i, y_{i+1}$ in the case of transition scores). \n",
    "\n",
    "### Features\n",
    "\n",
    "Today we will use two types of simple features. \n",
    "\n",
    "- **Features that mimic the features used by the HMM**\n",
    "    - This will allow us to directly compare the performance of a generative vs a discriminative approach\n",
    "    \n",
    "\n",
    "- **Features that are implicit in the HMM** which are simple indicatiors of the initial, transition, final and emission events.\n",
    "    - Given a certain position $i$ and state $c$ the set of features that mimic the HMM are:\n",
    "\n",
    "\n",
    "| Conditions to be met       |    Name             |\n",
    "| ----------------           | ----------------    |\n",
    "| $y_i=c  \\,\\, \\& \\,\\, i =0$        | Initial features    |\n",
    "| $y_i=c   \\,\\, \\& \\,\\, y_{i-1}=c$  | Transition features |\n",
    "| $y_i=c_k \\,\\, \\& \\,\\,  i=N$        | Final features      |\n",
    "| $x_i=w_j \\,\\, \\& \\,\\,  y_i=c_k$    | Emission features   |\n",
    "\n",
    "When we used a generative model we were forced to make some independence assumptions. However, since we are now in a discriminative approach,where we model $P(Y | X)$ rather than $P(X,Y)$ we are not tied anymore to some of these assumptions. In particular:\n",
    "\n",
    "- We may use “overlapping” features, e.g., features that fire simultaneously for many instances. For example, we can use a feature for a word, such as a feature which fires for the word ”brilliantly”, and another for prefixes and suffixes of that word, such as one which fires if the last two letters of the word are ”ly”. This would lead to an awkward model if we wanted to insist on a generative approach.\n",
    "\n",
    "\n",
    "- We may use features that depend arbitrarily on the entire input sequence $x$. On the other hand, we still need to resort to “local” features with respect to the outputs (e.g. looking only at consecutive state pairs), otherwise decoding algorithms will become more expensive.\n",
    "\n",
    "\n",
    "#### Typical features used for POS taggigng with discriminative models\n",
    "\n",
    "The following table shows some typical POS tagging features. Let us consider $P_{set}$ and $S_{set}$ to be two sets of prefixes and sufixes respectively (set by the user).\n",
    "\n",
    "\n",
    "| Conditions to be met for some of the most typical POS features     |    Name      |\n",
    "| ----------------                                | ----------------    |\n",
    "| $y_i=c , \\,\\,  i =0$                      | Initial features    |\n",
    "| $y_i=c ,\\,\\,  y_{i-1}=c$                | Transition features |\n",
    "| $y_i=c_k ,\\,\\, i=N$                     | Final features      |\n",
    "| $x_i=w_j ,\\,\\,  y_i=c_k$                 | Basic Emission features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ is uppercased } ,\\,\\,  y_i=c_k$                 | Upper case features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains digit} ,\\,\\,  y_i=c_k$                 | Digit features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains hyphen} ,\\,\\,  y_i=c_k$                 | Hypthen features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[0:i] \\in P_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Prefix features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[-i] \\in S_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Suffix features|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can have more complex features which look arbitrarily to the input sequence. We are not going to have them in this exercise only for performance reasons (to have less features and smaller caches). State-of-the-art sequence classifiers can easily reach over one million features!\n",
    "\n",
    "Our features subdivide in two groups\n",
    "\n",
    "- **node features**: $f_{\\text{emiss}}, f_{\\text{init}}, f_{\\text{final}}$. Node features depend only on a single position in the state sequence (or node in the trellis).\n",
    "    \n",
    "    \n",
    "- **edge features**: $f_{\\text{trans}}$. Edge features depend on two consecutive positions in the state sequence (an edge in the trellis)\n",
    "\n",
    "\n",
    "    \n",
    "| score  definitions: scalar product between features and weights |\n",
    "| ------------------------------- | ---------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}\\,(i,x,y_1) =\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}\\,(i, x, y_i)$ |\n",
    "|$\\textrm{score}_\\textrm{init}(x,y_1)=\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init} \\,(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}\\,(i,x,y_i,y_{i+1}) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}\\,(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}\\,(x,y_N) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}\\,(x, y_N)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Sequential Classifiers\n",
    "\n",
    "Given a weight vector $W$, the conditional probability $P_{W}(Y=y|X=x)$ is then defined as follows: \n",
    "\n",
    "\n",
    "$$\n",
    "P_{w}(Y=y \\vert X=x) = \\frac{1}{Z({w},x)}\\exp \\big( w \\cdot F_{\\mathrm{init}}\\,(x,y_1) +  \\sum_{i=1}^{N-1} w \\cdot F_{\\mathrm{trans}}\\,(i,x,y_i,y_{i+1}) + w \\cdot F_{\\mathrm{final}}\\,(x,y_N) + \\sum_{i=1}^{N} w \\cdot F_{\\mathrm{emiss}}\\,(i,x,y_i) \\big)\n",
    "$$\n",
    "\n",
    "where the normalizing factor $Z(w,x)$ is called the **partition function**:\n",
    "\n",
    "$$\n",
    "\\sum_{y\\in \\Lambda^N} \\exp \\big( w \\cdot F_{\\mathrm{init}}\\,(x,y_1) + \n",
    "\\sum_{i=1}^{N-1} w \\cdot F_{\\mathrm{trans}}\\,(i,x,y_i,y_{i+1}) + w \\cdot F_{\\mathrm{final}}\\,(x,y_N) + \\sum_{i=1}^{N} w \\cdot F_{\\mathrm{emiss}}\\,(i,x,y_i) \\big)\n",
    "$$\n",
    "\n",
    "#### training a discriminative sequential classifier\n",
    "\n",
    "\n",
    "\n",
    "For training,  the important problem is that of obtaining the weight vector $w$ that lead to an accurate \n",
    "classifier.  We will discuss two possible strategies:\n",
    "\n",
    "-  Maximizing conditional log-likelihood from a set of labeled data $\\{(x^m,y^m)\\}_{m=1}^M$, yielding **conditional random fields**. This corresponds to the following optimization problem:\n",
    "$$\n",
    "\\hat{w} = \\arg\\max_{w} \\sum_{m=1}^M \\log P_{w}(Y=y^m \\vert X=x^m).\n",
    "$$\n",
    "To avoid overfitting, it is common to regularize with the Euclidean norm function, \n",
    "which is equivalent to considering a zero-mean Gaussian prior on the weight vector.\n",
    "The problem becomes:\n",
    "$$\n",
    "\\hat{w} = \\arg\\max_{w} \\sum_{m=1}^M \\log P_{w}(Y=y^m | X=x^m) - \\frac{\\lambda}{2} \\|w\\|^2.\n",
    "$$\n",
    "This is precisely the structured variant of the maximum entropy \n",
    "method discussed in Chapter 1. Unlike HMMs, this problem does not have a closed form solution \n",
    "and has to be solved with numerical optimization. \n",
    "\n",
    "\n",
    "- Alternatively, running the **structured perceptron** algorithm \n",
    "to obtain a weight vector $w$ that accurately classifies the training data. \n",
    "We will see that this simple strategy achieves results which are competitive \n",
    "with conditional log-likelihood maximization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Decoding\n",
    "\n",
    "One important thing to notice is that the decoding process - the process by which we pick the most likely label $y_i$ for the observation $x_i$ - stays the same. This means *we do not need to develop new decoders,* only new functions to compute the scores. Because of this, we will keep using the Viterbi and Forward-Backward algorithms developed on Day 2.\n",
    "\n",
    "\n",
    "For decoding,   there are three important problems that need to be solved:\n",
    "\n",
    "1. Given $X=x$, compute the most likely output sequence $\\hat{y}$ (the one which maximizes $P_{w}(Y=y|X=x)$). \n",
    "2. Compute the posterior marginals $P_{w}(Y_i=y_i|X=x)$ at each position $i$.\n",
    "3. Evaluate the partition function $Z(w,x)$. \n",
    "\n",
    "Interestingly, all these problems can be solved by using the very same\n",
    "algorithms that were \n",
    "already implemented for HMMs: the Viterbi algorithm (for 1) and the forward-backward algorithm (for 2--3). All that changes is the way the scores are computed. \n",
    "\n",
    "\n",
    "\n",
    "### Training the classifier\n",
    "\n",
    "Today we will cover two different approaches to training sequential discriminative models. Given a training set with $M$ observation-label pairs, $\\{(x_m, y_m)\\}_{m=1}^M$ (note that $x_m$, $y_m$ are $N$-dimensional vectors, as $m$ indexes the training sample):\n",
    "\n",
    "* **Conditional Random Fields** maximize the log-likelihood over $w$ on the observed training set.\n",
    "* **Structured Perceptron** iteratively updates $w$ in order to correctly classify the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields\n",
    "\n",
    "CRFs are the generalization of the Maximum Entropy classifier for sequences. The general concept is the same, with a couple of diferences to be discussed below. They are trained by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{w}} = \\underset{\\boldsymbol{w}}{\\textrm{argmax}}\\ \\sum_{m=1}^M\\log P_\\boldsymbol{w}(Y=y_m\\,|\\,X=x_m)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "P_w(Y=y_m\\,|\\,X=x_m) =\n",
    "\\frac{1}{Z(w, x)}\\ \\exp \\big(\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}\\,(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}\\,(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w} \\cdot \\boldsymbol{f}_\\textrm{trans}\\,(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}\\,(x, y_N))\\\\\n",
    "Z(w, x) = \\sum_{y\\,\\in\\,\\Lambda^N} \\exp \\big(\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}\\,(i, x, y_i) +  \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}\\,(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}\\,(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}\\,(x, y_N) \\big)\n",
    "$$\n",
    "\n",
    "As before, the partition function $Z(w,x)$ ensures the sum of probabilities over all possible labels $y\\in\\Lambda^N$ is equal to 1.\n",
    "\n",
    "To avoid overfitting, it is common to add the Euclidean norm function as a regularization term. This is equivalent\n",
    "to considering a zero-mean Gaussian prior on the weight vector. The optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{w}} = \\underset{\\boldsymbol{w}}{\\textrm{arg max}}\\ \\sum_{m=1}^M\\log P_\\boldsymbol{w}(Y=y_m\\,|\\,X=x_m) - \\frac{\\lambda}{2}||\\boldsymbol{w}||^2\n",
    "$$\n",
    "\n",
    "which is precisely the structured variant of the maximum entropy method discussed on Day 1. Unlike with HMMs, the above problem has to be solved numerically.\n",
    "\n",
    "### Differences with respect to ME algorithm\n",
    "\n",
    "* CRe does not compute posterior marginals, $P(Y=y\\,|\\,X=x)$ for every possible $y\\in\\Lambda^N$, as there are exponentially many possible $y$'s. Instead, it decomposes the model into parts — nodes and edges — and computes the posteriors for those parts, that is, $P(Y_i=y_i\\,|\\,X=x)$ and $P(Y_i=y_i, Y_{i+1}=y_{i+1}\\,|\\,X=x)$. The crucial point is that these quantities can be computed using the forward-backward algorithm.\n",
    "\n",
    "* Instead of updating the features for all possible outputs $y′ ∈ \\Lambda^N$, we again exploit the decomposition into parts above and update only “local features” at the nodes and edges. [TODO clarify this]\n",
    "\n",
    "### Pseudo Code\n",
    "\n",
    "Below is pseudo code to optimize a CRF with the stochastic gradient descent (SGD) algorithm. Our toolkit also includes an implementation of a quasi-Newton method, L-BFGS, which converges faster. For the purpose of this exercise, however, we will stick with SGD.\n",
    "\n",
    "<img src=\"./images_for_notebooks/day_3/CRF_pseudocode.png\">\n",
    "\n",
    "\n",
    "\n",
    "<font color='red', size=5>\n",
    "Put information on why the expected value over parameters has the form it can be seen in the algorithm\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "### HMM vs CRF\n",
    "\n",
    "HMMs are factored linear models. HMM can be written as CRFs.\n",
    "\n",
    "If an HMM is a type of CRF...\n",
    "\n",
    "- HMM features are tied to the generative process\n",
    "- CRF features are very flexible. They can look at the whole input x paired with any labeled bigram.\n",
    "- In practise, for prediction taks, good discriminative features can improve accuracy al lot.\n",
    "\n",
    "##### Parameter estimation\n",
    "\n",
    "- HMMs focus on explaining the data, both x and y.\n",
    "- CRFs focus  on the mapping from x to y\n",
    "- A priori it is hard to say which paradigm is better.\n",
    "- Similar dilema can be found in the Naive Bayes vs. Maximum Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "Objectives:\n",
    "\n",
    "\n",
    "* train a CRF using different feature sets for part-of-speech tagging;\n",
    "\n",
    "* evaluate the model on the training, development and test sets.\n",
    "\n",
    "\n",
    "Files used:\n",
    "\n",
    "* class CRFOnline in lxmls/sequences/crf_online.py file\n",
    "\n",
    "* class PostagCorpus in lxmls/sequences/readers/pos_corpus.py file\n",
    "\n",
    "* class IDFeatures in lxmls/sequences/id_feature.py file\n",
    "\n",
    "* class ExtendedFeatures in lxmls/sequences/extended_feature.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CRF and Structured Perceptron are discriminative classifiers\n",
    "\n",
    "\n",
    "Classes that implement CRF and Structured Perceptron inherit from  ```lxmls.sequences.discriminative_sequence_classifier```\n",
    "\n",
    "More concretly the classes are\n",
    "\n",
    "- **```class StructuredPerceptron```** and can be found in ```lxmls.sequences.structured_perceptron.py ```      \n",
    "- **```class CRFBatch```**  and can be found in ```lxmls.sequences.crf_batch.py ```\n",
    "- **```class CRFOnline```**  and can be found in ```lxmls.sequences.crf_online.py ```\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## Code for ```lxmls.sequences.sequence_classifier ```\n",
    "\n",
    "\n",
    "#### What is important to notice in  SequenceClassifier class\n",
    "\n",
    "The code provided below in **(*)** defines an abstract class for a sequence classifier.\n",
    "\n",
    "It can be noticed that a ```SequenceClassifier``` has the methods\n",
    "\n",
    "- ```.train_supervised```: trains the algorithm in a supervised way\n",
    "- ```.compute_scores```: Computes the scores of a given sequence\n",
    "\n",
    "** Both of this methods are not implemented**\n",
    "\n",
    "```python        \n",
    "    # Code from SequenceClassifier\n",
    "    def train_supervised(self, sequence_list):\n",
    "        \"\"\" Train a classifier in a supervised setting.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        \"\"\" Compute emission and transition scores for the decoder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "```       \n",
    "\n",
    "\n",
    "** The exercise of today is related to the train_supervised method in the structured perceptron**.\n",
    "\n",
    "The code for the CRF will be all already implemented so there is no need to implement anything.\n",
    "The code for the structured perceptron has already the ```.train_supervised``` method implemented. Nevertheless, the ```.train_supervised``` inside the ```StructuredPerceptron``` class calls another function that you will have to implement. The function is ```.perceptron_update```\n",
    "\n",
    "\n",
    "\n",
    "#### (*) Code for the SequenceClassifier class\n",
    "\n",
    "```python        \n",
    "import sequence_classification_decoder as scd\n",
    "\n",
    "class SequenceClassifier:\n",
    "    \"\"\" Implements an abstract sequence classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_labels, state_labels):\n",
    "        \"\"\"Initialize a sequence classifier. observation_labels and\n",
    "        state_labels are the sets of observations and states, respectively.\n",
    "        They must be LabelDictionary objects.\"\"\"\n",
    "\n",
    "        self.decoder = scd.SequenceClassificationDecoder()\n",
    "        self.observation_labels = observation_labels\n",
    "        self.state_labels = state_labels\n",
    "        self.trained = False\n",
    "\n",
    "    def get_num_states(self):\n",
    "        \"\"\" Return the number of states.\"\"\"\n",
    "        return len(self.state_labels)\n",
    "\n",
    "    def get_num_observations(self):\n",
    "        \"\"\" Return the number of observations (e.g. word types).\"\"\"\n",
    "        return len(self.observation_labels)\n",
    "\n",
    "    def train_supervised(self, sequence_list):\n",
    "        \"\"\" Train a classifier in a supervised setting.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        \"\"\" Compute emission and transition scores for the decoder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "     .\n",
    "     .\n",
    "     .\n",
    "```\n",
    "\n",
    "\n",
    "## Code for ```DiscriminativeSequenceClassifier```\n",
    "\n",
    "\n",
    "The code for DiscriminativeSequenceClassifier class is as follows:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import lxmls.sequences.sequence_classifier as sc\n",
    "\n",
    "class DiscriminativeSequenceClassifier(sc.SequenceClassifier):\n",
    "\n",
    "    def __init__(self, observation_labels, state_labels, feature_mapper):\n",
    "        sc.SequenceClassifier.__init__(self, observation_labels, state_labels)\n",
    "\n",
    "        # Set feature mapper and initialize parameters.\n",
    "        self.feature_mapper = feature_mapper\n",
    "        self.parameters = np.zeros(self.feature_mapper.get_num_features())\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        num_states = self.get_num_states()\n",
    "        length = len(sequence.x)\n",
    "        emission_scores = np.zeros([length, num_states])\n",
    "        initial_scores = np.zeros(num_states)\n",
    "        transition_scores = np.zeros([length-1, num_states, num_states])\n",
    "        final_scores = np.zeros(num_states)\n",
    "\n",
    "        # Initial position.\n",
    "        for tag_id in xrange(num_states):\n",
    "            initial_features = self.feature_mapper.get_initial_features(sequence, tag_id)\n",
    "            score = 0.0\n",
    "            for feat_id in initial_features:\n",
    "                score += self.parameters[feat_id]\n",
    "            initial_scores[tag_id] = score\n",
    "\n",
    "        # Intermediate position.\n",
    "        for pos in xrange(length):\n",
    "            for tag_id in xrange(num_states):\n",
    "                emission_features = self.feature_mapper.get_emission_features(sequence, pos, tag_id)\n",
    "                score = 0.0\n",
    "                for feat_id in emission_features:\n",
    "                    score += self.parameters[feat_id]\n",
    "                emission_scores[pos, tag_id] = score\n",
    "            if pos > 0:\n",
    "                for tag_id in xrange(num_states):\n",
    "                    for prev_tag_id in xrange(num_states):\n",
    "                        transition_features = self.feature_mapper.get_transition_features(\n",
    "                            sequence, pos, tag_id, prev_tag_id)\n",
    "                        score = 0.0\n",
    "                        for feat_id in transition_features:\n",
    "                            score += self.parameters[feat_id]\n",
    "                        transition_scores[pos-1, tag_id, prev_tag_id] = score\n",
    "\n",
    "        # Final position.\n",
    "        for prev_tag_id in xrange(num_states):\n",
    "            final_features = self.feature_mapper.get_final_features(sequence, prev_tag_id)\n",
    "            score = 0.0\n",
    "            for feat_id in final_features:\n",
    "                score += self.parameters[feat_id]\n",
    "            final_scores[prev_tag_id] = score\n",
    "\n",
    "        return initial_scores, transition_scores, final_scores, emission_scores\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Feature Generation\n",
    "\n",
    "Given a dataset,\n",
    "\n",
    "in order to build the features\n",
    "\n",
    "- An instance from IDFeatures (we will call it feature_mapper) must be instanciated\n",
    "- feature_mapper.build_features() must be executed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data inside train-02-21.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = lxmls.readers.pos_corpus.PostagCorpus()\n",
    "data_path = \"../lxmls-toolkit/data/\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=10, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 samples in train_seq\n"
     ]
    }
   ],
   "source": [
    "print \"There are\", len(train_seq), \"samples in train_seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the IDFeatures class\n",
    "\n",
    "** IDFeatures object will be referred to as a  ```feature_mapper```.**\n",
    "\n",
    "\n",
    "We will assume feature_mapper has been instantiated with\n",
    "\n",
    "    feature_mapper = lxmls.sequences.id_feature.IDFeatures(train_seq)\n",
    "\n",
    "\n",
    "\n",
    "#### About feature_mappers\n",
    "A ```feature_mapper``` will contain the following attributes:\n",
    "\n",
    "- the dataset in ```.dataset```\n",
    "    - if we instantiate the feature mapper with a dataset X then ```feature_mapper.dataset```will be a copy of X\n",
    "\n",
    "\n",
    "- a dictionary of features in ```.feature_dict```\n",
    "    - this dictionary will default to ```{}```. \n",
    "    - In order to build the features the feature mapper must call ```.build_features()``` function.\n",
    "    \n",
    "    \n",
    "- a list of features in ```.feature_list```\n",
    "    - this list will default to ```[]```. \n",
    "    - In order to build the list of features the feature mapper must call ```.build_features()``` function.\n",
    "\n",
    "A ```feature_mapper``` will contain the method \n",
    "\n",
    "- A method to generate features, ```.build_features```\n",
    "    - this method will create features using the ```.dataset``.\n",
    "    - This method will also fill ```.feature_dict``` and ```.feature_list``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = lxmls.sequences.id_feature.IDFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_list',\n",
       " 'final_state_feature_cache',\n",
       " 'node_feature_cache',\n",
       " 'add_features',\n",
       " 'dataset',\n",
       " 'initial_state_feature_cache',\n",
       " 'feature_dict',\n",
       " 'edge_feature_cache']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ms./noun Haag/noun plays/verb Elianti/noun ./. ,\n",
       " The/det new/adj rate/noun will/verb be/verb payable/adj Feb./noun 15/num ./. ]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of features used is 2683 \n",
    "# This is the dimension d of  f(x,y)\n",
    "len(feature_mapper.feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building features using ```.build_features()```\n",
    "\n",
    "Now we will call ```feature_mapper.build_features()``` to get the features for each training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mapper = lxmls.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1000 samples in the features build from train_seq\n"
     ]
    }
   ],
   "source": [
    "print \"there are\", len(feature_mapper.feature_list), \"samples in the features build from train_seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature_mapper object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Examining initial, transition, final and emission features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0]], [[3], [5], [7], [9]], [[10]], [[1], [2], [4], [6], [8]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial features: [[0]]\n",
      "\n",
      "Transition features: [[3], [5], [7], [9]]\n",
      "\n",
      "Final features: [[10]]\n",
      "\n",
      "Emission features: [[1], [2], [4], [6], [8]]\n"
     ]
    }
   ],
   "source": [
    "print \"\\nInitial features:\",     feature_mapper.feature_list[0][0]\n",
    "print \"\\nTransition features:\",  feature_mapper.feature_list[0][1]\n",
    "print \"\\nFinal features:\",       feature_mapper.feature_list[0][2]\n",
    "print \"\\nEmission features:\",    feature_mapper.feature_list[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codification of the features\n",
    "\n",
    "All features are saved in ``feature_mapper.feature_dict`` this represents our feature vector. If it is our feature vector why it's not a vector? Good point! In order to make the algorithm fast, the code is written using dicts, so if we access only a few positions from the dict and compute substractions it will be much faster than computing the substraction of two huge weight vectors.\n",
    "\n",
    "Features are identifyed by **init_tag:**, **prev_tag:**,  **final_prev_tag:**, **id:**\n",
    "\n",
    "- **init_tag:** when they are Initial features\n",
    "    - Example: **``init_tag:noun``** is an initial feature that describes that the first word is a noun\n",
    "    \n",
    "    \n",
    "- **prev_tag:** when they are transition features\n",
    "    - Example: **``prev_tag:noun::noun``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a noun.\n",
    "    - Example: **``prev_tag:noun:.``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a `.` (this is usually foud as the last transition feature since most phrases will end up with a dot)\n",
    "      \n",
    "\n",
    "\n",
    "- **final_prev_tag:** when they are final features\n",
    "    - Example: **``final_prev_tag:.``** is a final feature stating that the last \"word\" in the sentence was a dot.\n",
    "\n",
    "\n",
    "- **id:** when they are emission features\n",
    "    - Example: **``id:plays::verb``** is an emission feature, describing that the current word is plays and the current hidden state is a verb.\n",
    "    - Example: **``id:Feb.::noun``** is an emission feature, describing that the current word is \"Feb.\" and the current hidden state is a noun.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inv_feature_dict = {word: pos for pos, word in feature_mapper.feature_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:noun']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_tag:noun::noun',\n",
       " 'prev_tag:noun::verb',\n",
       " 'prev_tag:verb::noun',\n",
       " 'prev_tag:noun::.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_prev_tag:.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id:Ms.::noun',\n",
       " u'id:Haag::noun',\n",
       " u'id:plays::verb',\n",
       " u'id:Elianti::noun',\n",
       " u'id:.::.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial features: [[11]]\n",
      "\n",
      "Transition features: [[14], [16], [5], [19], [21], [16], [24], [25]]\n",
      "\n",
      "Final features: [[10]]\n",
      "\n",
      "Emission features: [[12], [13], [15], [17], [18], [20], [22], [23], [8]]\n"
     ]
    }
   ],
   "source": [
    "print \"\\nInitial features:\",     feature_mapper.feature_list[1][0]\n",
    "print \"\\nTransition features:\",  feature_mapper.feature_list[1][1]\n",
    "print \"\\nFinal features:\",       feature_mapper.feature_list[1][2]\n",
    "print \"\\nEmission features:\",    feature_mapper.feature_list[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[11]],\n",
       " [[14], [16], [5], [19], [21], [16], [24], [25]],\n",
       " [[10]],\n",
       " [[12], [13], [15], [17], [18], [20], [22], [23], [8]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:det']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_tag:det::adj',\n",
       " 'prev_tag:adj::noun',\n",
       " 'prev_tag:noun::verb',\n",
       " 'prev_tag:verb::verb',\n",
       " 'prev_tag:verb::adj',\n",
       " 'prev_tag:adj::noun',\n",
       " 'prev_tag:noun::num',\n",
       " 'prev_tag:num::.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_prev_tag:.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id:The::det',\n",
       " u'id:new::adj',\n",
       " u'id:rate::noun',\n",
       " u'id:will::verb',\n",
       " u'id:be::verb',\n",
       " u'id:payable::adj',\n",
       " u'id:Feb.::noun',\n",
       " u'id:15::num',\n",
       " u'id:.::.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2683"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_dict[\"id:Each::det\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'init_tag:det'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_feature_dict [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = [inv_feature_dict[x]==\"id:Each::det\"  for x in range(len(inv_feature_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946 id:Each::det\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(inv_feature_dict)):\n",
    "    if inv_feature_dict[x]==\"id:Each::det\":\n",
    "        print x, inv_feature_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'init_tag:noun',\n",
       " 1: u'id:Ms.::noun',\n",
       " 2: u'id:Haag::noun',\n",
       " 3: 'prev_tag:noun::noun',\n",
       " 4: u'id:plays::verb',\n",
       " 5: 'prev_tag:noun::verb',\n",
       " 6: u'id:Elianti::noun',\n",
       " 7: 'prev_tag:verb::noun',\n",
       " 8: u'id:.::.',\n",
       " 9: 'prev_tag:noun::.',\n",
       " 10: 'final_prev_tag:.',\n",
       " 11: 'init_tag:det',\n",
       " 12: u'id:The::det',\n",
       " 13: u'id:new::adj',\n",
       " 14: 'prev_tag:det::adj',\n",
       " 15: u'id:rate::noun',\n",
       " 16: 'prev_tag:adj::noun',\n",
       " 17: u'id:will::verb',\n",
       " 18: u'id:be::verb',\n",
       " 19: 'prev_tag:verb::verb',\n",
       " 20: u'id:payable::adj',\n",
       " 21: 'prev_tag:verb::adj',\n",
       " 22: u'id:Feb.::noun',\n",
       " 23: u'id:15::num',\n",
       " 24: 'prev_tag:noun::num',\n",
       " 25: 'prev_tag:num::.',\n",
       " 26: u'id:A::det',\n",
       " 27: u'id:record::noun',\n",
       " 28: 'prev_tag:det::noun',\n",
       " 29: u'id:date::noun',\n",
       " 30: u'id:has::verb',\n",
       " 31: u\"id:n't::adv\",\n",
       " 32: 'prev_tag:verb::adv',\n",
       " 33: u'id:been::verb',\n",
       " 34: 'prev_tag:adv::verb',\n",
       " 35: u'id:set::verb',\n",
       " 36: 'prev_tag:verb::.',\n",
       " 37: 'init_tag:adv',\n",
       " 38: u'id:Not::adv',\n",
       " 39: u'id:all::det',\n",
       " 40: 'prev_tag:adv::det',\n",
       " 41: u'id:those::det',\n",
       " 42: 'prev_tag:det::det',\n",
       " 43: u'id:who::pron',\n",
       " 44: 'prev_tag:det::pron',\n",
       " 45: u'id:wrote::verb',\n",
       " 46: 'prev_tag:pron::verb',\n",
       " 47: u'id:oppose::verb',\n",
       " 48: u'id:the::det',\n",
       " 49: 'prev_tag:verb::det',\n",
       " 50: u'id:changes::noun',\n",
       " 51: u'id:Bach::noun',\n",
       " 52: u\"id:'s::prt\",\n",
       " 53: 'prev_tag:noun::prt',\n",
       " 54: u'id:``::.',\n",
       " 55: 'prev_tag:prt::.',\n",
       " 56: u'id:Air::noun',\n",
       " 57: 'prev_tag:.::noun',\n",
       " 58: u\"id:''::.\",\n",
       " 59: u'id:followed::verb',\n",
       " 60: 'prev_tag:.::verb',\n",
       " 61: 'init_tag:conj',\n",
       " 62: u'id:Or::conj',\n",
       " 63: u'id:was::verb',\n",
       " 64: 'prev_tag:conj::verb',\n",
       " 65: u'id:it::pron',\n",
       " 66: 'prev_tag:verb::pron',\n",
       " 67: u'id:because::adp',\n",
       " 68: 'prev_tag:pron::adp',\n",
       " 69: 'prev_tag:adp::noun',\n",
       " 70: u'id:Collins::noun',\n",
       " 71: u'id:had::verb',\n",
       " 72: u'id:gone::verb',\n",
       " 73: u'id:?::.',\n",
       " 74: 'init_tag:verb',\n",
       " 75: u'id:Is::verb',\n",
       " 76: u'id:this::det',\n",
       " 77: u'id:future::noun',\n",
       " 78: u'id:of::adp',\n",
       " 79: 'prev_tag:noun::adp',\n",
       " 80: u'id:chamber::noun',\n",
       " 81: u'id:music::noun',\n",
       " 82: 'init_tag:pron',\n",
       " 83: u'id:What::pron',\n",
       " 84: u\"id:'s::verb\",\n",
       " 85: u'id:next::adj',\n",
       " 86: 'prev_tag:adj::.',\n",
       " 87: u'id:Slides::noun',\n",
       " 88: u'id:to::prt',\n",
       " 89: u'id:illustrate::verb',\n",
       " 90: 'prev_tag:prt::verb',\n",
       " 91: u'id:Shostakovich::noun',\n",
       " 92: u'id:quartets::noun',\n",
       " 93: u'id:And::conj',\n",
       " 94: u'id:their::pron',\n",
       " 95: 'prev_tag:conj::pron',\n",
       " 96: u'id:suspicions::noun',\n",
       " 97: 'prev_tag:pron::noun',\n",
       " 98: u'id:each::det',\n",
       " 99: 'prev_tag:adp::det',\n",
       " 100: u'id:other::adj',\n",
       " 101: u'id:run::verb',\n",
       " 102: 'prev_tag:adj::verb',\n",
       " 103: u'id:deep::adv',\n",
       " 104: 'prev_tag:adv::.',\n",
       " 105: 'init_tag:.',\n",
       " 106: u'id:Old-time::adj',\n",
       " 107: 'prev_tag:.::adj',\n",
       " 108: u'id:kiddies::noun',\n",
       " 109: u'id:,::.',\n",
       " 110: 'prev_tag:.::.',\n",
       " 111: u'id:he::pron',\n",
       " 112: 'prev_tag:.::pron',\n",
       " 113: u'id:says::verb',\n",
       " 114: u'id:Perhaps::adv',\n",
       " 115: u'id:not::adv',\n",
       " 116: 'prev_tag:conj::adv',\n",
       " 117: u'id:just::adv',\n",
       " 118: 'prev_tag:adv::adv',\n",
       " 119: u'id:for::adp',\n",
       " 120: 'prev_tag:adv::adp',\n",
       " 121: u'id:players::noun',\n",
       " 122: u'id:My::pron',\n",
       " 123: u'id:fastball::noun',\n",
       " 124: u'id:is::verb',\n",
       " 125: u'id:good::adj',\n",
       " 126: u'id:I::pron',\n",
       " 127: u'id:tried::verb',\n",
       " 128: u'id:But::conj',\n",
       " 129: 'prev_tag:conj::det',\n",
       " 130: u'id:ballplayers::noun',\n",
       " 131: u'id:disagree::verb',\n",
       " 132: 'init_tag:adj',\n",
       " 133: u'id:Most::adj',\n",
       " 134: u'id:are::verb',\n",
       " 135: u'id:trim::adj',\n",
       " 136: u'id:there::det',\n",
       " 137: 'prev_tag:det::verb',\n",
       " 138: u'id:pride::noun',\n",
       " 139: u'id:So::adv',\n",
       " 140: 'prev_tag:adv::pron',\n",
       " 141: u'id:adjusts::verb',\n",
       " 142: u'id:He::pron',\n",
       " 143: u'id:no::adv',\n",
       " 144: 'prev_tag:pron::adv',\n",
       " 145: u'id:longer::adv',\n",
       " 146: u'id:crowds::verb',\n",
       " 147: u'id:plate::noun',\n",
       " 148: u'id:expect::verb',\n",
       " 149: u'id:slower::adj',\n",
       " 150: u'id:fastballs::noun',\n",
       " 151: u'id:Its::pron',\n",
       " 152: u'id:maximum::adj',\n",
       " 153: 'prev_tag:pron::adj',\n",
       " 154: u'id:velocity::noun',\n",
       " 155: u'id:72::num',\n",
       " 156: 'prev_tag:verb::num',\n",
       " 157: u'id:mph::noun',\n",
       " 158: 'prev_tag:num::noun',\n",
       " 159: u'id:worried::adj',\n",
       " 160: 'prev_tag:adv::adj',\n",
       " 161: u'id:control::noun',\n",
       " 162: u'id:Terms::noun',\n",
       " 163: u'id:were::verb',\n",
       " 164: u'id:disclosed::verb',\n",
       " 165: u'id:Tuesday::noun',\n",
       " 166: u'id:October::noun',\n",
       " 167: u'id:31::num',\n",
       " 168: u'id:1989::num',\n",
       " 169: 'prev_tag:.::num',\n",
       " 170: 'final_prev_tag:num',\n",
       " 171: u'id:PRIME::adj',\n",
       " 172: u'id:RATE::noun',\n",
       " 173: u'id::::.',\n",
       " 174: u'id:10::num',\n",
       " 175: u'id:1\\\\/2::num',\n",
       " 176: 'prev_tag:num::num',\n",
       " 177: u'id:%::noun',\n",
       " 178: u'id:DISCOUNT::noun',\n",
       " 179: u'id:7::num',\n",
       " 180: u'id:minimum::adj',\n",
       " 181: u'id:unit::noun',\n",
       " 182: u'id:$::.',\n",
       " 183: u'id:100,000::num',\n",
       " 184: u'id:Source::noun',\n",
       " 185: u'id:Telerate::noun',\n",
       " 186: u'id:Systems::noun',\n",
       " 187: u'id:Inc::noun',\n",
       " 188: u'id:MERRILL::noun',\n",
       " 189: u'id:LYNCH::noun',\n",
       " 190: u'id:READY::noun',\n",
       " 191: u'id:ASSETS::noun',\n",
       " 192: u'id:TRUST::noun',\n",
       " 193: u'id:8.63::num',\n",
       " 194: u'id:Output::noun',\n",
       " 195: u'id:goods-producing::adj',\n",
       " 196: 'prev_tag:adp::adj',\n",
       " 197: u'id:industries::noun',\n",
       " 198: u'id:increased::verb',\n",
       " 199: u'id:0.1::num',\n",
       " 200: u'id:venture::noun',\n",
       " 201: u'id:based::verb',\n",
       " 202: u'id:in::adp',\n",
       " 203: 'prev_tag:verb::adp',\n",
       " 204: u'id:Indianapolis::noun',\n",
       " 205: u'id:Mr.::noun',\n",
       " 206: u'id:Tomash::noun',\n",
       " 207: u'id:remain::verb',\n",
       " 208: u'id:as::adp',\n",
       " 209: u'id:a::det',\n",
       " 210: u'id:director::noun',\n",
       " 211: u'id:emeritus::noun',\n",
       " 212: u'id:Rubendall::noun',\n",
       " 213: u'id:could::verb',\n",
       " 214: u'id:reached::verb',\n",
       " 215: u'id:index::noun',\n",
       " 216: u'id:fell::verb',\n",
       " 217: u'id:109.85::num',\n",
       " 218: u'id:Monday::noun',\n",
       " 219: u'id:Institutional::adj',\n",
       " 220: u'id:investors::noun',\n",
       " 221: u'id:mostly::adv',\n",
       " 222: 'prev_tag:noun::adv',\n",
       " 223: u'id:remained::verb',\n",
       " 224: u'id:on::adp',\n",
       " 225: u'id:sidelines::noun',\n",
       " 226: u'id:Sumitomo::noun',\n",
       " 227: u'id:Realty::noun',\n",
       " 228: u'id:&::conj',\n",
       " 229: 'prev_tag:noun::conj',\n",
       " 230: u'id:Development::noun',\n",
       " 231: 'prev_tag:conj::noun',\n",
       " 232: u'id:rose::verb',\n",
       " 233: u'id:40::num',\n",
       " 234: 'prev_tag:num::prt',\n",
       " 235: u'id:2170::num',\n",
       " 236: 'prev_tag:prt::num',\n",
       " 237: u'id:Heiwa::noun',\n",
       " 238: u'id:Real::noun',\n",
       " 239: u'id:Estate::noun',\n",
       " 240: u'id:gained::verb',\n",
       " 241: u'id:2210::num',\n",
       " 242: u'id:Investor::noun',\n",
       " 243: u'id:focus::noun',\n",
       " 244: u'id:shifted::verb',\n",
       " 245: u'id:quickly::adv',\n",
       " 246: u'id:traders::noun',\n",
       " 247: u'id:said::verb',\n",
       " 248: u'id:No::det',\n",
       " 249: u'id:one::noun',\n",
       " 250: u'id:wants::verb',\n",
       " 251: u'id:stock::noun',\n",
       " 252: 'prev_tag:adp::pron',\n",
       " 253: u'id:books::noun',\n",
       " 254: u'id:Taipei::noun',\n",
       " 255: u'id:closed::verb',\n",
       " 256: u'id:holiday::noun',\n",
       " 257: u'id:percentage::noun',\n",
       " 258: u'id:change::noun',\n",
       " 259: u'id:since::adp',\n",
       " 260: u'id:year-end::noun',\n",
       " 261: u'id:1980::num',\n",
       " 262: 'prev_tag:adp::num',\n",
       " 263: u'id:equaling::verb',\n",
       " 264: 'prev_tag:num::verb',\n",
       " 265: u'id:100::num',\n",
       " 266: 'init_tag:adp',\n",
       " 267: u'id:For::adp',\n",
       " 268: u'id:longer-term::adj',\n",
       " 269: u'id:CDs::noun',\n",
       " 270: u'id:yields::noun',\n",
       " 271: u'id:up::prt',\n",
       " 272: 'prev_tag:verb::prt',\n",
       " 273: u'id:In::adp',\n",
       " 274: u'id:France::noun',\n",
       " 275: u'id:!::.',\n",
       " 276: u'id:(::.',\n",
       " 277: u'id:still::adv',\n",
       " 278: u'id:say::verb',\n",
       " 279: u'id:do::verb',\n",
       " 280: u'id:look::verb',\n",
       " 281: u'id:down::adv',\n",
       " 282: u'id:At::adp',\n",
       " 283: u'id:least::adj',\n",
       " 284: 'prev_tag:adj::adv',\n",
       " 285: u'id:when::adv',\n",
       " 286: u'id:you::pron',\n",
       " 287: u'id:ascending::verb',\n",
       " 288: u'id:)::.',\n",
       " 289: u\"id:'m::verb\",\n",
       " 290: u'id:talking::verb',\n",
       " 291: u'id:about::adp',\n",
       " 292: u'id:landing::noun',\n",
       " 293: u'id:canal::noun',\n",
       " 294: u'id:porous::adj',\n",
       " 295: u'id:wicker::noun',\n",
       " 296: u'id:basket::noun',\n",
       " 297: u'id:With::adp',\n",
       " 298: u'id:pilot::noun',\n",
       " 299: 'prev_tag:noun::pron',\n",
       " 300: u'id:speaks::verb',\n",
       " 301: u'id:no::det',\n",
       " 302: u'id:English::noun',\n",
       " 303: u'id:wonder::noun',\n",
       " 304: u'id:We::pron',\n",
       " 305: u'id:coming::verb',\n",
       " 306: u'id:straight::adv',\n",
       " 307: u'id:into::adp',\n",
       " 308: u'id:neither::adv',\n",
       " 309: u'id:can::verb',\n",
       " 310: u'id:your::pron',\n",
       " 311: u'id:Which::det',\n",
       " 312: u'id:makes::verb',\n",
       " 313: u'id:chase::noun',\n",
       " 314: u'id:car::noun',\n",
       " 315: u'id:necessary::adj',\n",
       " 316: 'prev_tag:noun::adj',\n",
       " 317: u'id:looked::verb',\n",
       " 318: u'id:at::adp',\n",
       " 319: u'id:my::pron',\n",
       " 320: u'id:watch::noun',\n",
       " 321: u'id:Barely::adv',\n",
       " 322: u'id:half-an-hour::noun',\n",
       " 323: 'prev_tag:adv::noun',\n",
       " 324: u'id:aloft::adv',\n",
       " 325: u'id:de::noun',\n",
       " 326: u'id:Vries::noun',\n",
       " 327: u'id:free-lance::adj',\n",
       " 328: u'id:writer::noun',\n",
       " 329: u'id:Fed::noun',\n",
       " 330: u'id:spokesman::noun',\n",
       " 331: u'id:denied::verb',\n",
       " 332: u'id:LaFalce::noun',\n",
       " 333: u'id:statement::noun',\n",
       " 334: 'prev_tag:prt::noun',\n",
       " 335: u'id:board::noun',\n",
       " 336: u'id:by::adp',\n",
       " 337: u'id:one::num',\n",
       " 338: u'id:26::num',\n",
       " 339: u'id:members::noun',\n",
       " 340: u'id:issue::noun',\n",
       " 341: u'id:stickier::adj',\n",
       " 342: u'id:than::adp',\n",
       " 343: 'prev_tag:adj::adp',\n",
       " 344: u'id:seems::verb',\n",
       " 345: u'id:Defining::verb',\n",
       " 346: u'id:combat::noun',\n",
       " 347: u'id:aircraft::noun',\n",
       " 348: u'id:even::adv',\n",
       " 349: u'id:tougher::adj',\n",
       " 350: u'id:Accounting::noun',\n",
       " 351: u'id:problems::noun',\n",
       " 352: u'id:raise::verb',\n",
       " 353: u'id:more::adj',\n",
       " 354: u'id:knotty::adj',\n",
       " 355: 'prev_tag:adj::adj',\n",
       " 356: u'id:issues::noun',\n",
       " 357: u'id:Saul::noun',\n",
       " 358: u'id:Resnick::noun',\n",
       " 359: 'final_prev_tag:noun',\n",
       " 360: u'id:Vice::noun',\n",
       " 361: u'id:President::noun',\n",
       " 362: u'id:Public::noun',\n",
       " 363: u'id:Affairs::noun',\n",
       " 364: u'id:Senate::noun',\n",
       " 365: u'id:probably::adv',\n",
       " 366: u'id:vote::verb',\n",
       " 367: u'id:long::adv',\n",
       " 368: u'id:afterward::adv',\n",
       " 369: u'id:Gerald::noun',\n",
       " 370: u'id:F.::noun',\n",
       " 371: u'id:Seib::noun',\n",
       " 372: u'id:contributed::verb',\n",
       " 373: 'prev_tag:prt::det',\n",
       " 374: u'id:article::noun',\n",
       " 375: u'id:company::noun',\n",
       " 376: u'id:manufacturing::verb',\n",
       " 377: u'id:carpet::noun',\n",
       " 378: u'id:1967::num',\n",
       " 379: u'id:price::noun',\n",
       " 380: u'id:This::det',\n",
       " 381: u'id:measure::noun',\n",
       " 382: u'id:dropped::verb',\n",
       " 383: u'id:sharply::adv',\n",
       " 384: u'id:August::noun',\n",
       " 385: u'id:remainder::noun',\n",
       " 386: u'id:downturn::noun',\n",
       " 387: u'id:begin::verb',\n",
       " 388: u'id:sometime::adv',\n",
       " 389: 'final_prev_tag:adp',\n",
       " 390: u'id:Pilgrim::noun',\n",
       " 391: u'id:closed::adj',\n",
       " 392: u'id:32::num',\n",
       " 393: u'id:months::noun',\n",
       " 394: u'id:Bids::noun',\n",
       " 395: u'id:totaling::verb',\n",
       " 396: u'id:515::num',\n",
       " 397: u'id:million::num',\n",
       " 398: u'id:submitted::verb',\n",
       " 399: u'id:475::num',\n",
       " 400: u'id:It::pron',\n",
       " 401: u'id:also::adv',\n",
       " 402: u'id:unnecessary::adj',\n",
       " 403: u'id:They::pron',\n",
       " 404: u'id:promised::verb',\n",
       " 405: u'id:yet::adv',\n",
       " 406: u'id:really::adv',\n",
       " 407: 'prev_tag:adp::adv',\n",
       " 408: u'id:stuff::noun',\n",
       " 409: u'id:Finding::verb',\n",
       " 410: u'id:him::pron',\n",
       " 411: u'id:became::verb',\n",
       " 412: u'id:an::det',\n",
       " 413: u'id:obsession::noun',\n",
       " 414: u'id:Stoll::noun',\n",
       " 415: u'id:Some::det',\n",
       " 416: u'id:nights::noun',\n",
       " 417: u'id:slept::verb',\n",
       " 418: u'id:under::adp',\n",
       " 419: u'id:his::pron',\n",
       " 420: u'id:desk::noun',\n",
       " 421: u'id:His::pron',\n",
       " 422: u'id:boss::noun',\n",
       " 423: u'id:complained::verb',\n",
       " 424: u'id:neglect::noun',\n",
       " 425: u'id:chores::noun',\n",
       " 426: u'id:Finally::adv',\n",
       " 427: u'id:got::verb',\n",
       " 428: u'id:help::noun',\n",
       " 429: u'id:Tymnet::noun',\n",
       " 430: u'id:major::adj',\n",
       " 431: u'id:network::noun',\n",
       " 432: u'id:linking::verb',\n",
       " 433: u'id:computers::noun',\n",
       " 434: u'id:angry::adj',\n",
       " 435: u'id:return::noun',\n",
       " 436: u'id:Melloan::noun',\n",
       " 437: u'id:deputy::adj',\n",
       " 438: u'id:editor::noun',\n",
       " 439: u'id:Journal::noun',\n",
       " 440: u'id:UNIFIRST::noun',\n",
       " 441: u'id:Corp.::noun',\n",
       " 442: u'id:declared::verb',\n",
       " 443: u'id:2-for-1::adj',\n",
       " 444: u'id:split::noun',\n",
       " 445: u'id:dividend::noun',\n",
       " 446: u'id:five::num',\n",
       " 447: u'id:cents::noun',\n",
       " 448: 'prev_tag:noun::det',\n",
       " 449: u'id:share::noun',\n",
       " 450: u'id:Reserve::noun',\n",
       " 451: u'id:Fund::noun',\n",
       " 452: 'prev_tag:det::adp',\n",
       " 453: u'id:allegations::noun',\n",
       " 454: u'id:simply::adv',\n",
       " 455: u'id:bizarre::adj',\n",
       " 456: u'id:sharp::adj',\n",
       " 457: u'id:tack::noun',\n",
       " 458: u'id:Nobody::noun',\n",
       " 459: u'id:guts::noun',\n",
       " 460: u'id:complain::verb',\n",
       " 461: u'id:Certainly::adv',\n",
       " 462: u'id:lawyers::noun',\n",
       " 463: u'id:inquiry::noun',\n",
       " 464: u'id:soon::adv',\n",
       " 465: u'id:focused::verb',\n",
       " 466: u'id:judge::noun',\n",
       " 467: u'id:Later::adv',\n",
       " 468: 'prev_tag:.::det',\n",
       " 469: u'id:went::verb',\n",
       " 470: u'id:step::noun',\n",
       " 471: u'id:farther::adv',\n",
       " 472: u'id:again::adv',\n",
       " 473: u'id:bank::noun',\n",
       " 474: u'id:acquiesced::verb',\n",
       " 475: u'id:should::verb',\n",
       " 476: u'id:scream::noun',\n",
       " 477: u'id:William::noun',\n",
       " 478: u'id:S.::noun',\n",
       " 479: u'id:Smith::noun',\n",
       " 480: u'id:Virginia::noun',\n",
       " 481: u'id:M.W.::noun',\n",
       " 482: u'id:Gardiner::noun',\n",
       " 483: u'id:Continental::noun',\n",
       " 484: u'id:Cablevision::noun',\n",
       " 485: u'id:Inc.::noun',\n",
       " 486: u'id:--::.',\n",
       " 487: u'id:Beatrice::noun',\n",
       " 488: u'id:Co.::noun',\n",
       " 489: u'id:coupon::noun',\n",
       " 490: u'id:13::num',\n",
       " 491: u'id:3\\\\/4::num',\n",
       " 492: u'id:New::noun',\n",
       " 493: u'id:Jersey::noun',\n",
       " 494: u'id:Wastewater::noun',\n",
       " 495: u'id:Treatment::noun',\n",
       " 496: u'id:Trust::noun',\n",
       " 497: u'id:Matagorda::noun',\n",
       " 498: u'id:County::noun',\n",
       " 499: u'id:Navigation::noun',\n",
       " 500: u'id:District::noun',\n",
       " 501: u'id:No.::noun',\n",
       " 502: u'id:1::num',\n",
       " 503: u'id:Texas::noun',\n",
       " 504: u'id:Federal::noun',\n",
       " 505: u'id:Home::noun',\n",
       " 506: u'id:Loan::noun',\n",
       " 507: u'id:Mortgage::noun',\n",
       " 508: u'id:Complete::adj',\n",
       " 509: u'id:details::noun',\n",
       " 510: u'id:immediately::adv',\n",
       " 511: u'id:available::adj',\n",
       " 512: u'id:Lomas::noun',\n",
       " 513: u'id:Funding::noun',\n",
       " 514: u'id:II::noun',\n",
       " 515: u'id:J.C.::noun',\n",
       " 516: u'id:Penney::noun',\n",
       " 517: u'id:Diesel::noun',\n",
       " 518: u'id:Kiki::noun',\n",
       " 519: u'id:Co::noun',\n",
       " 520: u'id:Japan::noun',\n",
       " 521: u'id:Chugoku::noun',\n",
       " 522: u'id:Electric::noun',\n",
       " 523: u'id:Power::noun',\n",
       " 524: u'id:Fees::noun',\n",
       " 525: u'id:7\\\\/8::num',\n",
       " 526: u'id:Okobank::noun',\n",
       " 527: u'id:Finland::noun',\n",
       " 528: 'init_tag:x',\n",
       " 529: u'id:First::x',\n",
       " 530: 'prev_tag:x::.',\n",
       " 531: u'id:they::pron',\n",
       " 532: u'id:safe::adj',\n",
       " 533: u'id:Second::x',\n",
       " 534: u'id:liquid::adj',\n",
       " 535: u'id:Third::x',\n",
       " 536: u'id:offer::verb',\n",
       " 537: u'id:high::adj',\n",
       " 538: u'id:All::det',\n",
       " 539: u'id:concerns::noun',\n",
       " 540: u'id:Toronto::noun',\n",
       " 541: u'id:Rockefeller::noun',\n",
       " 542: u'id:investment::noun',\n",
       " 543: u'id:its::pron',\n",
       " 544: u'id:largest::adj',\n",
       " 545: u'id:`::.',\n",
       " 546: u'id:Frequent::adj',\n",
       " 547: u'id:Drinker::noun',\n",
       " 548: u\"id:'::.\",\n",
       " 549: u'id:Offer::noun',\n",
       " 550: u'id:Stirs::verb',\n",
       " 551: u'id:Up::prt',\n",
       " 552: u'id:Spirited::adj',\n",
       " 553: 'prev_tag:prt::adj',\n",
       " 554: u'id:Debate::noun',\n",
       " 555: u'id:Chivas::noun',\n",
       " 556: u'id:Class::noun',\n",
       " 557: u'id:first::adj',\n",
       " 558: u'id:such::adj',\n",
       " 559: u'id:promotion::noun',\n",
       " 560: u'id:Goya::noun',\n",
       " 561: u'id:Concocts::verb',\n",
       " 562: u'id:Milk::noun',\n",
       " 563: u'id:Hispanic::adj',\n",
       " 564: u'id:Tastes::noun',\n",
       " 565: u'id:Jewelry::noun',\n",
       " 566: u'id:Makers::noun',\n",
       " 567: u'id:Copy::verb',\n",
       " 568: u'id:Cosmetics::noun',\n",
       " 569: u'id:Sales::noun',\n",
       " 570: u'id:Ploys::noun',\n",
       " 571: u'id:merchandise::noun',\n",
       " 572: u'id:well::x',\n",
       " 573: 'prev_tag:.::x',\n",
       " 574: u'id:fake::adj',\n",
       " 575: u'id:limits::noun',\n",
       " 576: u'id:Her::pron',\n",
       " 577: u'id:idea::noun',\n",
       " 578: u'id:bring::verb',\n",
       " 579: u'id:in::prt',\n",
       " 580: u'id:live::adj',\n",
       " 581: u'id:zoo::noun',\n",
       " 582: u'id:animals::noun',\n",
       " 583: u'id:Odds::noun',\n",
       " 584: u'id:and::conj',\n",
       " 585: u'id:Ends::noun',\n",
       " 586: u'id:cholesterol::noun',\n",
       " 587: 'prev_tag:.::adp',\n",
       " 588: u'id:course::noun',\n",
       " 589: u'id:reset::verb',\n",
       " 590: u'id:opening::adj',\n",
       " 591: u'id:arguments::noun',\n",
       " 592: u'id:today::noun',\n",
       " 593: u'id:She::pron',\n",
       " 594: u'id:now::adv',\n",
       " 595: u'id:lives::verb',\n",
       " 596: u'id:with::adp',\n",
       " 597: u'id:relatives::noun',\n",
       " 598: u'id:Alabama::noun',\n",
       " 599: u'id:Then::adv',\n",
       " 600: u'id:would::verb',\n",
       " 601: u'id:move::verb',\n",
       " 602: u'id:movement::noun',\n",
       " 603: u'id:Europe::noun',\n",
       " 604: u'id:that::det',\n",
       " 605: 'prev_tag:adv::prt',\n",
       " 606: u'id:Their::pron',\n",
       " 607: u'id:legacy::noun',\n",
       " 608: u'id:on::prt',\n",
       " 609: u'id:Yet::adv',\n",
       " 610: u'id:these::det',\n",
       " 611: u'id:purchases::noun',\n",
       " 612: u'id:misleading::adj',\n",
       " 613: u'id:South::noun',\n",
       " 614: u'id:Korea::noun',\n",
       " 615: u'id:continue::verb',\n",
       " 616: u'id:profitable::adj',\n",
       " 617: u'id:Panda::noun',\n",
       " 618: u'id:Motors::noun',\n",
       " 619: 'prev_tag:num::adj',\n",
       " 620: u'id:NUCLEAR::noun',\n",
       " 621: u'id:REACTOR::noun',\n",
       " 622: u'id:FOR::adp',\n",
       " 623: u'id:ISRAEL::noun',\n",
       " 624: u'id:two::num',\n",
       " 625: 'prev_tag:det::num',\n",
       " 626: u'id:signing::verb',\n",
       " 627: u'id:trade::noun',\n",
       " 628: u'id:agreement::noun',\n",
       " 629: u'id:Ashurst::noun',\n",
       " 630: 'prev_tag:adj::prt',\n",
       " 631: u'id:Far::noun',\n",
       " 632: u'id:East::noun',\n",
       " 633: u'id:NEW::noun',\n",
       " 634: u'id:JERSEY::noun',\n",
       " 635: u'id:MERGER::noun',\n",
       " 636: u'id:DRUG::noun',\n",
       " 637: u'id:WARS::noun',\n",
       " 638: u'id:level::noun',\n",
       " 639: u'id:did::verb',\n",
       " 640: u'id:doors::noun',\n",
       " 641: u'id:town-watching::adj',\n",
       " 642: u'id:excursions::noun',\n",
       " 643: u'id:downright::adv',\n",
       " 644: u'id:comic::adj',\n",
       " 645: u'id:Other::adj',\n",
       " 646: u'id:trips::noun',\n",
       " 647: u'id:more::adv',\n",
       " 648: u'id:productive::adj',\n",
       " 649: u'id:Why::adv',\n",
       " 650: u'id:Accord::noun',\n",
       " 651: u'id:prices::noun',\n",
       " 652: u'id:start::verb',\n",
       " 653: 'prev_tag:adp::.',\n",
       " 654: u'id:12,345::num',\n",
       " 655: u'id:George::noun',\n",
       " 656: u'id:Bush::noun',\n",
       " 657: u'id:own::adj',\n",
       " 658: u'id:Peter::noun',\n",
       " 659: u'id:Gumbel::noun',\n",
       " 660: u'id:Moscow::noun',\n",
       " 661: u'id:There::det',\n",
       " 662: u'id:better::adj',\n",
       " 663: u'id:ways::noun',\n",
       " 664: u'id:promote::verb',\n",
       " 665: u'id:cause::noun',\n",
       " 666: u'id:Here::adv',\n",
       " 667: u'id:cases::noun',\n",
       " 668: u'id:dies::verb',\n",
       " 669: u'id:maybe::adv',\n",
       " 670: u'id:TV::noun',\n",
       " 671: u'id:lose::verb',\n",
       " 672: u'id:nothing::noun',\n",
       " 673: u'id:States::noun',\n",
       " 674: u'id:following::verb',\n",
       " 675: u'id:suit::noun',\n",
       " 676: u'id:California::noun',\n",
       " 677: u'id:enacted::verb',\n",
       " 678: u'id:rights::noun',\n",
       " 679: u'id:law::noun',\n",
       " 680: u'id:1988::num',\n",
       " 681: u'id:HUGO::noun',\n",
       " 682: u'id:FELLED::verb',\n",
       " 683: u'id:vast::adj',\n",
       " 684: u'id:timberlands::noun',\n",
       " 685: u'id:BRIEFS::noun',\n",
       " 686: u'id:Industry::noun',\n",
       " 687: u'id:executives::noun',\n",
       " 688: u'id:wishing::verb',\n",
       " 689: u'id:Achenbaum::noun',\n",
       " 690: u'id:well::adv',\n",
       " 691: u'id:Cotton::noun',\n",
       " 692: u'id:Campaign::noun',\n",
       " 693: u'id:Frank::noun',\n",
       " 694: u'id:Mingo::noun',\n",
       " 695: u'id:Dies::verb',\n",
       " 696: u'id:49::num',\n",
       " 697: u'id:Clients::noun',\n",
       " 698: u'id:include::verb',\n",
       " 699: u'id:Miller::noun',\n",
       " 700: u'id:Brewing::noun',\n",
       " 701: u'id:General::noun',\n",
       " 702: u'id:Ad::noun',\n",
       " 703: u'id:Notes::noun',\n",
       " 704: u'id:...::.',\n",
       " 705: u'id:EARNINGS::noun',\n",
       " 706: u'id:Excerpts::noun',\n",
       " 707: u'id:follow::verb',\n",
       " 708: u'id:already::adv',\n",
       " 709: u'id:industrialized::adj',\n",
       " 710: u'id:zero-sum::adj',\n",
       " 711: u'id:game::noun',\n",
       " 712: u'id:That::det',\n",
       " 713: u'id:possible::adj',\n",
       " 714: u'id:On::adp',\n",
       " 715: u'id:U.S.-Japan::adj',\n",
       " 716: u'id:relations::noun',\n",
       " 717: u'id:encouraged::adj',\n",
       " 718: u'id:understand::verb',\n",
       " 719: 'prev_tag:pron::.',\n",
       " 720: u'id:relationships::noun',\n",
       " 721: u'id:British::noun',\n",
       " 722: u'id:totally::adv',\n",
       " 723: u'id:different::adj',\n",
       " 724: u'id:lorded::verb',\n",
       " 725: u'id:over::adp',\n",
       " 726: u'id:me::pron',\n",
       " 727: u'id:some::det',\n",
       " 728: 'prev_tag:pron::det',\n",
       " 729: u'id:themselves::pron',\n",
       " 730: u'id:mean::verb',\n",
       " 731: u'id:normal::adj',\n",
       " 732: u'id:adult::noun',\n",
       " 733: u'id:relationship::noun',\n",
       " 734: u'id:trouble::noun',\n",
       " 735: u'id:over::prt',\n",
       " 736: u'id:ca::verb',\n",
       " 737: u'id:way::noun',\n",
       " 738: u'id:forward::adv',\n",
       " 739: u'id:Let::verb',\n",
       " 740: u\"id:'s::pron\",\n",
       " 741: u'id:put::verb',\n",
       " 742: u'id:bluntly::adv',\n",
       " 743: u'id:First::noun',\n",
       " 744: u'id:Boston::noun',\n",
       " 745: u'id:sole::adj',\n",
       " 746: u'id:underwriter::noun',\n",
       " 747: u'id:trust::noun',\n",
       " 748: u'id:issue::verb',\n",
       " 749: u'id:certificates::noun',\n",
       " 750: u'id:service::verb',\n",
       " 751: u'id:receivables::noun',\n",
       " 752: u'id:Mexico::noun',\n",
       " 753: u'id:urgently::adv',\n",
       " 754: u'id:needs::verb',\n",
       " 755: u'id:Salinas::noun',\n",
       " 756: u'id:big::adj',\n",
       " 757: u'id:inflows::noun',\n",
       " 758: 'prev_tag:.::adv',\n",
       " 759: u'id:If::adp',\n",
       " 760: u'id:Opinion::noun',\n",
       " 761: u'id:mixed::adj',\n",
       " 762: u'id:three-month::adj',\n",
       " 763: u'id:prospects::noun',\n",
       " 764: u'id:Estimated::adj',\n",
       " 765: u'id:volume::noun',\n",
       " 766: u'id:3.5::num',\n",
       " 767: u'id:ounces::noun',\n",
       " 768: u'id:Edelson::noun',\n",
       " 769: u'id:comment::noun',\n",
       " 770: u'id:lot::noun',\n",
       " 771: u'id:needed::verb',\n",
       " 772: u'id:done::verb',\n",
       " 773: u'id:security::noun',\n",
       " 774: u'id:business::noun',\n",
       " 775: u'id:favorite::adj',\n",
       " 776: u'id:subject::noun',\n",
       " 777: u'id:love::verb',\n",
       " 778: u'id:Officials::noun',\n",
       " 779: u'id:Temple::noun',\n",
       " 780: u'id:declined::verb',\n",
       " 781: u'id:comment::verb',\n",
       " 782: u'id:FHA::noun',\n",
       " 783: u'id:program::noun',\n",
       " 784: u'id:hemorrhaging::verb',\n",
       " 785: u'id:bad::adj',\n",
       " 786: u'id:loans::noun',\n",
       " 787: u'id:Gillette::noun',\n",
       " 788: u'id:Africa::noun',\n",
       " 789: u'id:employs::verb',\n",
       " 790: u'id:about::adv',\n",
       " 791: u'id:250::num',\n",
       " 792: 'prev_tag:adv::num',\n",
       " 793: u'id:people::noun',\n",
       " 794: u'id:vote::noun',\n",
       " 795: u'id:approve::verb',\n",
       " 796: 'final_prev_tag:verb',\n",
       " 797: u'id:Trial::noun',\n",
       " 798: u'id:Terror::noun',\n",
       " 799: u'id:Arnold::noun',\n",
       " 800: u'id:J.::noun',\n",
       " 801: u'id:Zarett::noun',\n",
       " 802: u'id:Rodeo::noun',\n",
       " 803: u'id:applause::noun',\n",
       " 804: u'id:broncs::noun',\n",
       " 805: u'id:cheer::noun',\n",
       " 806: u'id:Marvin::noun',\n",
       " 807: u'id:Alisky::noun',\n",
       " 808: u'id:bottom-line::adj',\n",
       " 809: u'id:administration::noun',\n",
       " 810: u'id:lacks::verb',\n",
       " 811: u'id:comprehensive::adj',\n",
       " 812: u'id:health-care::noun',\n",
       " 813: u'id:policy::noun',\n",
       " 814: u'id:expected::verb',\n",
       " 815: u'id:report::verb',\n",
       " 816: u'id:summer::noun',\n",
       " 817: u'id:1991::num',\n",
       " 818: 'prev_tag:conj::num',\n",
       " 819: u'id:window::noun',\n",
       " 820: u'id:action::noun',\n",
       " 821: u'id:pressure::noun',\n",
       " 822: u'id:rise::verb',\n",
       " 823: u'id:costs::noun',\n",
       " 824: u'id:Limiting::verb',\n",
       " 825: u'id:care::noun',\n",
       " 826: u'id:wo::verb',\n",
       " 827: u'id:easy::adj',\n",
       " 828: u'id:or::conj',\n",
       " 829: 'prev_tag:adj::conj',\n",
       " 830: u'id:popular::adj',\n",
       " 831: 'prev_tag:conj::adj',\n",
       " 832: u'id:AFL-CIO::noun',\n",
       " 833: u'id:embraces::verb',\n",
       " 834: u'id:treatment::noun',\n",
       " 835: u'id:guidelines::noun',\n",
       " 836: u'id:steelmaker::noun',\n",
       " 837: u'id:16,000::num',\n",
       " 838: u'id:12.3::num',\n",
       " 839: u'id:shares::noun',\n",
       " 840: u'id:outstanding::adj',\n",
       " 841: u'id:Consolidation::noun',\n",
       " 842: u'id:overdue::adj',\n",
       " 843: u'id:such::det',\n",
       " 844: u'id:combination::noun',\n",
       " 845: u'id:presents::verb',\n",
       " 846: u'id:great::adj',\n",
       " 847: u'id:risks::noun',\n",
       " 848: u'id:days::noun',\n",
       " 849: u'id:respond::verb',\n",
       " 850: u'id:Nekoosa::noun',\n",
       " 851: u'id:incorporated::verb',\n",
       " 852: u'id:Maine::noun',\n",
       " 853: u'id:International::noun',\n",
       " 854: u'id:Paper::noun',\n",
       " 855: u'id:Weyerhaeuser::noun',\n",
       " 856: u'id:Bond::noun',\n",
       " 857: u'id:edged::verb',\n",
       " 858: u'id:higher::adv',\n",
       " 859: u'id:what::pron',\n",
       " 860: u'id:happened::verb',\n",
       " 861: u'id:yesterday::noun',\n",
       " 862: u'id:market::noun',\n",
       " 863: u'id:activity::noun',\n",
       " 864: u'id:Stock::noun',\n",
       " 865: u'id:rallied::verb',\n",
       " 866: u'id:active::adj',\n",
       " 867: u'id:trading::noun',\n",
       " 868: u'id:dollar::noun',\n",
       " 869: u'id:against::adp',\n",
       " 870: u'id:most::adj',\n",
       " 871: u'id:foreign::adj',\n",
       " 872: u'id:currencies::noun',\n",
       " 873: u'id:Nasdaq::noun',\n",
       " 874: u'id:7.08::num',\n",
       " 875: u'id:445.23::num',\n",
       " 876: u'id:Intel::noun',\n",
       " 877: u'id:up::adv',\n",
       " 878: u'id:3\\\\/8::num',\n",
       " 879: u'id:33::num',\n",
       " 880: u'id:Bank::noun',\n",
       " 881: u'id:Index::noun',\n",
       " 882: u'id:0.17::num',\n",
       " 883: u'id:432.78::num',\n",
       " 884: u'id:Gen-Probe::noun',\n",
       " 885: u'id:another::det',\n",
       " 886: u'id:takeover::noun',\n",
       " 887: u'id:MCI::noun',\n",
       " 888: u'id:Communications::noun',\n",
       " 889: u'id:added::verb',\n",
       " 890: u'id:43::num',\n",
       " 891: u'id:value::noun',\n",
       " 892: u'id:transaction::noun',\n",
       " 893: u'id:Santa::noun',\n",
       " 894: u'id:Fe::noun',\n",
       " 895: u'id:worth::adp',\n",
       " 896: u'id:analysts::noun',\n",
       " 897: u'id:bullish::adj',\n",
       " 898: u'id:Business::noun',\n",
       " 899: u'id:Railroad::noun',\n",
       " 900: u'id:natural::adj',\n",
       " 901: u'id:resources::noun',\n",
       " 902: u'id:real::adj',\n",
       " 903: u'id:estate::noun',\n",
       " 904: u'id:Year::noun',\n",
       " 905: u'id:ended::verb',\n",
       " 906: u'id:Dec.::noun',\n",
       " 907: u'id:Revenue::noun',\n",
       " 908: u'id:3.14::num',\n",
       " 909: u'id:billion::num',\n",
       " 910: u'id:Third::adj',\n",
       " 911: u'id:quarter::noun',\n",
       " 912: u'id:Sept.::noun',\n",
       " 913: u'id:30::num',\n",
       " 914: u'id:Average::adj',\n",
       " 915: u'id:daily::adj',\n",
       " 916: u'id:344,354::num',\n",
       " 917: u'id:widely::adv',\n",
       " 918: u'id:that::adp',\n",
       " 919: u'id:Second-tier::adj',\n",
       " 920: u'id:companies::noun',\n",
       " 921: u'id:receiving::verb',\n",
       " 922: u'id:less::adj',\n",
       " 923: u'id:per::adp',\n",
       " 924: u'id:ton::noun',\n",
       " 925: u'id:Commercial::adj',\n",
       " 926: u'id:gene-splicing::noun',\n",
       " 927: u'id:born::verb',\n",
       " 928: u'id:Armstrong::noun',\n",
       " 929: u'id:1\\\\/8::num',\n",
       " 930: u'id:39::num',\n",
       " 931: u'id:ERC::noun',\n",
       " 932: u'id:12::num',\n",
       " 933: u'id:Ogden::noun',\n",
       " 934: u'id:1\\\\/4::num',\n",
       " 935: u'id:Volume::noun',\n",
       " 936: u'id:totaled::verb',\n",
       " 937: u'id:11,820,000::num',\n",
       " 938: u'id:simultaneous::adj',\n",
       " 939: u'id:announcement::noun',\n",
       " 940: u'id:made::verb',\n",
       " 941: u'id:Big::noun',\n",
       " 942: u'id:Board::noun',\n",
       " 943: u'id:officials::noun',\n",
       " 944: u'id:publicly::adv',\n",
       " 945: u'id:work::verb',\n",
       " 946: u'id:Each::det',\n",
       " 947: u'id:agenda::noun',\n",
       " 948: u\"id:'ve::verb\",\n",
       " 949: u'id:dictation::noun',\n",
       " 950: u'id:wrecking::verb',\n",
       " 951: 'prev_tag:adp::verb',\n",
       " 952: u'id:them::pron',\n",
       " 953: u'id:exchange::noun',\n",
       " 954: u'id:take::verb',\n",
       " 955: u'id:pro-active::adj',\n",
       " 956: u'id:position::noun',\n",
       " 957: u'id:Craig::noun',\n",
       " 958: u'id:Torres::noun',\n",
       " 959: u'id:bond::noun',\n",
       " 960: u'id:higher::adj',\n",
       " 961: u'id:Health-insurance::noun',\n",
       " 962: u'id:soared::verb',\n",
       " 963: u'id:Markets::noun',\n",
       " 964: u'id:Stocks::noun',\n",
       " 965: u'id:176,100,000::num',\n",
       " 966: u'id:Corporate::adj',\n",
       " 967: u'id:bonds::noun',\n",
       " 968: u'id:unchanged::adj',\n",
       " 969: u'id:Treasury::noun',\n",
       " 970: u'id:Securities::noun',\n",
       " 971: u'id:light::adj',\n",
       " 972: u'id:Short-term::adj',\n",
       " 973: u'id:rates::noun',\n",
       " 974: u'id:Issues::noun',\n",
       " 975: u'id:Carmichael::noun',\n",
       " 976: u'id:demanded::verb',\n",
       " 977: u'id:stricter::adj',\n",
       " 978: u'id:convenants::noun',\n",
       " 979: u'id:Mortgage-Backed::adj',\n",
       " 980: u'id:Municipal::adj',\n",
       " 981: u'id:Foreign::adj',\n",
       " 982: u'id:Bonds::noun',\n",
       " 983: u'id:government::noun',\n",
       " 984: u'id:markets::noun',\n",
       " 985: u'id:quiet::adj',\n",
       " 986: u'id:Japanese::adj',\n",
       " 987: u'id:showed::verb',\n",
       " 988: u'id:little::adj',\n",
       " 989: u'id:GASB::noun',\n",
       " 990: u'id:rules::noun',\n",
       " 991: u'id:apply::verb',\n",
       " 992: u'id:units::noun',\n",
       " 993: u'id:valued::verb',\n",
       " 994: u'id:800::num',\n",
       " 995: u'id:Joseph::noun',\n",
       " 996: u'id:Granville::noun',\n",
       " 997: u'id:expects::verb',\n",
       " 998: u'id:Stovall::noun',\n",
       " 999: u'id:does::verb',\n",
       " ...}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Exercise 3.1 - Basic feature set\n",
    "\n",
    "_Start by training the model. You will receive feedback when each epoch is finished. Note that running the 20 epochs might take a while._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../lxmls-toolkit/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "corpus = pcc.PostagCorpus()\n",
    "\n",
    "# Load the training, test and development sequences\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=10, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"/test-23.conll\",\n",
    "                                           max_sent_len=10, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"/dev-22.conll\", \n",
    "                                          max_sent_len=10, max_nr_sent=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40, 43, 44, 41]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 6, 0, 4]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -5.779018\n",
      "Epoch: 1 Objective value: -3.192724\n",
      "Epoch: 2 Objective value: -2.717537\n",
      "Epoch: 3 Objective value: -2.436614\n",
      "Epoch: 4 Objective value: -2.240491\n",
      "Epoch: 5 Objective value: -2.091833\n",
      "Epoch: 6 Objective value: -1.973353\n",
      "Epoch: 7 Objective value: -1.875643\n",
      "Epoch: 8 Objective value: -1.793034\n",
      "Epoch: 9 Objective value: -1.721857\n",
      "Epoch: 10 Objective value: -1.659605\n",
      "Epoch: 11 Objective value: -1.604499\n",
      "Epoch: 12 Objective value: -1.555229\n",
      "Epoch: 13 Objective value: -1.510806\n",
      "Epoch: 14 Objective value: -1.470468\n",
      "Epoch: 15 Objective value: -1.433612\n",
      "Epoch: 16 Objective value: -1.399759\n",
      "Epoch: 17 Objective value: -1.368518\n",
      "Epoch: 18 Objective value: -1.339566\n",
      "Epoch: 19 Objective value: -1.312636\n"
     ]
    }
   ],
   "source": [
    "# Build features\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "# Train the model\n",
    "# You will receive feedback when each epoch is finished.\n",
    "# Note that running the 20 epochs might take a while.\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)\n",
    "\n",
    "\n",
    "# You will receive feedback when each epoch is finished, note that running the 20 epochs might take a while. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous cell execution should give the following results\n",
    "\n",
    "    Epoch: 0 Objective value: -5.779018\n",
    "    Epoch: 1 Objective value: -3.192724\n",
    "    Epoch: 2 Objective value: -2.717537\n",
    "    Epoch: 3 Objective value: -2.436614\n",
    "    Epoch: 4 Objective value: -2.240491\n",
    "    Epoch: 5 Objective value: -2.091833\n",
    "    Epoch: 6 Objective value: -1.973353\n",
    "    Epoch: 7 Objective value: -1.875643\n",
    "    Epoch: 8 Objective value: -1.793034\n",
    "    Epoch: 9 Objective value: -1.721857\n",
    "    Epoch: 10 Objective value: -1.659605\n",
    "    Epoch: 11 Objective value: -1.604499\n",
    "    Epoch: 12 Objective value: -1.555229\n",
    "    Epoch: 13 Objective value: -1.510806\n",
    "    Epoch: 14 Objective value: -1.470468\n",
    "    Epoch: 15 Objective value: -1.433612\n",
    "    Epoch: 16 Objective value: -1.399759\n",
    "    Epoch: 17 Objective value: -1.368518\n",
    "    Epoch: 18 Objective value: -1.339566\n",
    "    Epoch: 19 Objective value: -1.312636\n",
    "\n",
    "\n",
    "After training is done, evaluate the learned model on the training, development and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF -  Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print \"CRF -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your output should be similar to this:**\n",
    "\n",
    "    CRF -  Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n",
    "\n",
    "Compare with the results achieved with the HMM model (0.837 on the test set). Even when using a similar feature set, a CRF yields better results than the HMM from the previous lecture.\n",
    "\n",
    "Perform some error analysis and figure out what are the main errors the tagger is making. Compare them with the errors made by the HMM model.\n",
    "\n",
    "**Hint:** use the methods developed in the previous lecture to help you with the error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 - Extended feature set\n",
    "\n",
    "**Exercise 3.2 Repeat the previous exercise using the extended feature set. Compare the results**\n",
    "\n",
    "_Train the model again, this time using the extended feature set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.extended_feature as exfc\n",
    "import lxmls.sequences.crf_online as crfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build features\n",
    "feature_mapper_ext = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper_ext.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard feature_mapper has 2683  features\n",
      "The extended feature_mapper has 7261  features\n"
     ]
    }
   ],
   "source": [
    "print \"The standard feature_mapper has\", len(feature_mapper.feature_dict), \" features\"\n",
    "print \"The extended feature_mapper has\", len(feature_mapper_ext.feature_dict), \" features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper_ext.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# You will receive feedback when each epoch is finished.\n",
    "# Note that running the 20 epochs might take a while.\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -7.141596\n",
      "Epoch: 1 Objective value: -1.807511\n",
      "Epoch: 2 Objective value: -1.218877\n",
      "Epoch: 3 Objective value: -0.955739\n",
      "Epoch: 4 Objective value: -0.807821\n",
      "Epoch: 5 Objective value: -0.712858\n",
      "Epoch: 6 Objective value: -0.647382\n",
      "Epoch: 7 Objective value: -0.599442\n",
      "Epoch: 8 Objective value: -0.562584\n",
      "Epoch: 9 Objective value: -0.533411\n",
      "Epoch: 10 Objective value: -0.509885\n",
      "Epoch: 11 Objective value: -0.490548\n",
      "Epoch: 12 Objective value: -0.474318\n",
      "Epoch: 13 Objective value: -0.460438\n",
      "Epoch: 14 Objective value: -0.448389\n",
      "Epoch: 15 Objective value: -0.437800\n",
      "Epoch: 16 Objective value: -0.428402\n",
      "Epoch: 17 Objective value: -0.419990\n",
      "Epoch: 18 Objective value: -0.412406\n",
      "Epoch: 19 Objective value: -0.405524\n"
     ]
    }
   ],
   "source": [
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous cell should give the following results\n",
    "\n",
    "    Epoch: 0 Objective value: -7.141596\n",
    "    Epoch: 1 Objective value: -1.807511\n",
    "    Epoch: 2 Objective value: -1.218877\n",
    "    Epoch: 3 Objective value: -0.955739\n",
    "    Epoch: 4 Objective value: -0.807821\n",
    "    Epoch: 5 Objective value: -0.712858\n",
    "    Epoch: 6 Objective value: -0.647382\n",
    "    Epoch: 7 Objective value: -0.599442\n",
    "    Epoch: 8 Objective value: -0.562584\n",
    "    Epoch: 9 Objective value: -0.533411\n",
    "    Epoch: 10 Objective value: -0.509885\n",
    "    Epoch: 11 Objective value: -0.490548\n",
    "    Epoch: 12 Objective value: -0.474318\n",
    "    Epoch: 13 Objective value: -0.460438\n",
    "    Epoch: 14 Objective value: -0.448389\n",
    "    Epoch: 15 Objective value: -0.437800\n",
    "    Epoch: 16 Objective value: -0.428402\n",
    "    Epoch: 17 Objective value: -0.419990\n",
    "    Epoch: 18 Objective value: -0.412406\n",
    "    Epoch: 19 Objective value: -0.405524\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_ext -  Accuracy Train: 0.984 Dev: 0.899 Test: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print \"CRF_ext -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output of the previous cell should be similar to this:\n",
    "\n",
    "    CRF_ext -  Accuracy Train: 0.984 Dev: 0.899 Test: 0.894\n",
    "\n",
    "Compare the errors obtained with the two different feature sets. \n",
    "\n",
    "- Do some error analysis: what errors were correct by using more features? \n",
    "\n",
    "- Can you think of other features to use to solve the errors you found?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The main lesson from this exercise is that, if you are not satisfied by the accuracy of your algorithm, you can perform some error analysis and find out which errors your algorithm is making. You can then add more features which attempt to improve those specific errors — this is known as feature engineering.**\n",
    "\n",
    "\n",
    "\n",
    "#### End Ex 3.2   ----------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### About adding features \n",
    "\n",
    "Adding engineered features can lead to two problems:\n",
    "* More features will make training and decoding more expensive. For example, if you add features that depend on the current word and the previous word, the number of new features is the square of the number of different words, which is quite large. For example, the Penn Treebank has around 40000 different words, so you are adding a lot of new features, even though not all pairs of words will ever occur. Features that depend on three words (previous, current, and next) are even more numerous.\n",
    "\n",
    "* If features are very specific, such as the (previous word, current word, next word) one just mentioned, they might occur very rarely in the training set, which leads to overfit problems. Some of these problems (not all) can be mitigated with techniques such as smoothing, which you already learned about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Perceptron\n",
    "\n",
    "The structured perceptron, namely its averaged version, is a very simple\n",
    "algorithm that relies on Viterbi decoding and very simple additive\n",
    "updates. In practice this algorithm is very easy to implement and\n",
    "behaves remarkably well in a variety of problems. These two\n",
    "characteristics make the structured perceptron algorithm a natural\n",
    "first choice to try and test a new problem or a new feature set. \n",
    "\n",
    "<img src=\"./images_for_notebooks/day_3/structured_perceptron.png\">\n",
    "\n",
    "\n",
    "\n",
    "There are only two differences, which mimic the ones already seen for the comparison between CRFs \n",
    "and multi-class ME models:\n",
    "\n",
    "- Instead of explicitly enumerating all possible output  configurations (exponentially many of them) to compute \n",
    " $$\\widehat{y} := \\text{argmax}_{y'\\in\\mathcal{Y}} W \\cdot F(x^m,y')$$, \n",
    "it finds the best sequence through the Viterbi algorithm. \n",
    "\n",
    "\n",
    "- Instead of updating the features for the entire $\\widehat{y}$, \n",
    "it updates only the node and edge features at the positions where the\n",
    "  labels are different i.e., where mistakes are made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3 - Algorithm implementation\n",
    "\n",
    "**Implement the structured perceptron algorithm.**\n",
    "\n",
    "**To do this, edit file ```sequences/structured_perceptron.py``` and implement the function ```.perceptron_updates```**\n",
    "\n",
    "         \n",
    "This function should apply one round of the perceptron algorithm, updating the weights for a given sequence, and returning the number of predicted labels (which equals the sequence length) and the number of mistaken labels.\n",
    "\n",
    "Hint: You can try to adapt the function\n",
    "\n",
    "    def gradient_update(self, sequence, eta):\n",
    "\n",
    "\n",
    "You will need to replace the computation of posterior marginals by the Viterbi algorithm, and to change the parameter updates according to Algorithm 11. Note the role of the functions\n",
    "\n",
    "    self.feature_mapper.get_*_features()\n",
    "\n",
    "in providing the indices for the features obtained for $f(x^m,y^m)$ or f$(x^m,\\hat{y}^m )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Nothing to do, ex 3.3 is about implementing the perceptron update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4 - POS Tagging\n",
    "\n",
    "**Repeat Exercises 3.1–3.2 using the structured perceptron algorithm instead of a CRF. Report the results.**\n",
    "\n",
    "\n",
    "\n",
    "### Part 1 Run the structured perceptron with the standard feature_mapper\n",
    "Here is the code for the simple feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mapper = lxmls.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.structured_perceptron as spc\n",
    "\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.656806\n",
      "Epoch: 1 Accuracy: 0.820898\n",
      "Epoch: 2 Accuracy: 0.879176\n",
      "Epoch: 3 Accuracy: 0.907432\n",
      "Epoch: 4 Accuracy: 0.925239\n",
      "Epoch: 5 Accuracy: 0.939956\n",
      "Epoch: 6 Accuracy: 0.946284\n",
      "Epoch: 7 Accuracy: 0.953790\n",
      "Epoch: 8 Accuracy: 0.958499\n",
      "Epoch: 9 Accuracy: 0.955114\n",
      "Epoch: 10 Accuracy: 0.959235\n",
      "Epoch: 11 Accuracy: 0.968065\n",
      "Epoch: 12 Accuracy: 0.968212\n",
      "Epoch: 13 Accuracy: 0.966740\n",
      "Epoch: 14 Accuracy: 0.971302\n",
      "Epoch: 15 Accuracy: 0.968653\n",
      "Epoch: 16 Accuracy: 0.970419\n",
      "Epoch: 17 Accuracy: 0.971891\n",
      "Epoch: 18 Accuracy: 0.971744\n",
      "Epoch: 19 Accuracy: 0.973510\n"
     ]
    }
   ],
   "source": [
    "sp.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The execution of the previous cell should print\n",
    "    Epoch: 0 Accuracy: 0.656806\n",
    "    Epoch: 1 Accuracy: 0.820898\n",
    "    Epoch: 2 Accuracy: 0.879176\n",
    "    Epoch: 3 Accuracy: 0.907432\n",
    "    Epoch: 4 Accuracy: 0.925239\n",
    "    Epoch: 5 Accuracy: 0.939956\n",
    "    Epoch: 6 Accuracy: 0.946284\n",
    "    Epoch: 7 Accuracy: 0.953790\n",
    "    Epoch: 8 Accuracy: 0.958499\n",
    "    Epoch: 9 Accuracy: 0.955114\n",
    "    Epoch: 10 Accuracy: 0.959235\n",
    "    Epoch: 11 Accuracy: 0.968065\n",
    "    Epoch: 12 Accuracy: 0.968212\n",
    "    Epoch: 13 Accuracy: 0.966740\n",
    "    Epoch: 14 Accuracy: 0.971302\n",
    "    Epoch: 15 Accuracy: 0.968653\n",
    "    Epoch: 16 Accuracy: 0.970419\n",
    "    Epoch: 17 Accuracy: 0.971891\n",
    "    Epoch: 18 Accuracy: 0.971744\n",
    "    Epoch: 19 Accuracy: 0.973510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP -  Accuracy Train: 0.984 Dev: 0.835 Test: 0.840\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = sp.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp.evaluate_corpus(test_seq, pred_test)\n",
    "print \"SP -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution of the previous cell should print\n",
    "\n",
    "    SP -  Accuracy Train: 0.984 Dev: 0.835 Test: 0.840\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Run the structured perceptron with the standard feature_mapper_ext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build features\n",
    "# import lxmls.sequences.extended_feature as exfc\n",
    "feature_mapper_ext = lxmls.sequences.extended_feature .ExtendedFeatures(train_seq)\n",
    "feature_mapper_ext.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.structured_perceptron as spc\n",
    "\n",
    "sp_ext = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper_ext)\n",
    "sp_ext.num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.764386\n",
      "Epoch: 1 Accuracy: 0.872701\n",
      "Epoch: 2 Accuracy: 0.903458\n",
      "Epoch: 3 Accuracy: 0.927594\n",
      "Epoch: 4 Accuracy: 0.938484\n",
      "Epoch: 5 Accuracy: 0.951141\n",
      "Epoch: 6 Accuracy: 0.949816\n",
      "Epoch: 7 Accuracy: 0.959529\n",
      "Epoch: 8 Accuracy: 0.957616\n",
      "Epoch: 9 Accuracy: 0.962325\n",
      "Epoch: 10 Accuracy: 0.961148\n",
      "Epoch: 11 Accuracy: 0.970567\n",
      "Epoch: 12 Accuracy: 0.968212\n",
      "Epoch: 13 Accuracy: 0.973216\n",
      "Epoch: 14 Accuracy: 0.974393\n",
      "Epoch: 15 Accuracy: 0.973951\n",
      "Epoch: 16 Accuracy: 0.976600\n",
      "Epoch: 17 Accuracy: 0.977483\n",
      "Epoch: 18 Accuracy: 0.974834\n",
      "Epoch: 19 Accuracy: 0.977042\n"
     ]
    }
   ],
   "source": [
    "sp_ext.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution of the previous cell should print\n",
    "    Epoch: 0 Accuracy: 0.764386\n",
    "    Epoch: 1 Accuracy: 0.872701\n",
    "    Epoch: 2 Accuracy: 0.903458\n",
    "    Epoch: 3 Accuracy: 0.927594\n",
    "    Epoch: 4 Accuracy: 0.938484\n",
    "    Epoch: 5 Accuracy: 0.951141\n",
    "    Epoch: 6 Accuracy: 0.949816\n",
    "    Epoch: 7 Accuracy: 0.959529\n",
    "    Epoch: 8 Accuracy: 0.957616\n",
    "    Epoch: 9 Accuracy: 0.962325\n",
    "    Epoch: 10 Accuracy: 0.961148\n",
    "    Epoch: 11 Accuracy: 0.970567\n",
    "    Epoch: 12 Accuracy: 0.968212\n",
    "    Epoch: 13 Accuracy: 0.973216\n",
    "    Epoch: 14 Accuracy: 0.974393\n",
    "    Epoch: 15 Accuracy: 0.973951\n",
    "    Epoch: 16 Accuracy: 0.976600\n",
    "    Epoch: 17 Accuracy: 0.977483\n",
    "    Epoch: 18 Accuracy: 0.974834\n",
    "    Epoch: 19 Accuracy: 0.977042\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP_ext -  Accuracy Train: 0.984 Dev: 0.888 Test: 0.890\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp_ext.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp_ext.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp_ext.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = sp_ext.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp_ext.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp_ext.evaluate_corpus(test_seq, pred_test)\n",
    "print \"SP_ext -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The execution of the previous cell should print\n",
    "    SP_ext - Accuracy Train: 0.984 Dev: 0.888 Test: 0.890\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Summary\n",
    "\n",
    "\n",
    "| Model   | Train acc | Dev acc | Test acc |\n",
    "| --------| --------- | ------- |--------- |\n",
    "| crf     |  0.949    | 0.846   |   0.858  |\n",
    "| crf_ext |  0.984    | 0.899   |   0.894  |\n",
    "| sp      |  0.984    | 0.835   |   0.840  |\n",
    "| sp_ext  |  0.984    | 0.888   |   0.890  |\n",
    "\n",
    "\n",
    "\n",
    "CRF -  Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n",
    "\n",
    "CRF_ext -  Accuracy Train: 0.984 Dev: 0.899 Test: 0.894\n",
    "\n",
    "SP -  Accuracy Train: 0.984 Dev: 0.835 Test: 0.840\n",
    "\n",
    "SP_ext Train: 0.984 Dev: 0.835 Test: 0.840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
